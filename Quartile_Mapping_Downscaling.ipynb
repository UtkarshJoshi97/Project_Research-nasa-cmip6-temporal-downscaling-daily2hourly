{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c80bd7a2-c7cb-4398-bee9-f05ba66c0a1c",
   "metadata": {},
   "source": [
    "# Quartile Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337a5753-d270-473d-9731-1bfabe8fa697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Configuration for Quantile Mapping ---\n",
      "\n",
      "--- Step 2: Loading and Filtering all necessary data files for Minneapolis ---\n",
      "  Successfully loaded and filtered historical NASA data for Minneapolis (2000-2020).\n",
      "  Successfully loaded and filtered validation NASA data for Minneapolis.\n",
      "  All data loading complete.\n",
      "\n",
      "--- Step 3: Preparing daily summaries for the calibration period (2000-01-01 to 2020-12-31) ---\n",
      "  Daily summaries created and columns aligned.\n",
      "\n",
      "--- Step 4: Applying Quantile Mapping to validation data ---\n",
      "  Processing variable: 'tas' -> 'air_temperature_k'\n",
      "  Processing variable: 'tasmin' -> 'tasmin'\n",
      "  Processing variable: 'tasmax' -> 'tasmax'\n",
      "  Processing variable: 'sfcWind' -> 'wind_speed_ms'\n",
      "  Processing variable: 'hurs' -> 'relative_humidity_percent'\n",
      "  Processing variable: 'rsds' -> 'solar_radiation_w_m2'\n",
      "  Processing variable: 'rlds' -> 'thermal_radiation_w_m2'\n",
      "  Processing variable: 'precip_daily_mm' -> 'precip_hourly_mm'\n",
      "  Quantile mapping applied successfully.\n",
      "\n",
      "--- Step 5: Saving bias-corrected data to C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\\NASA_Corrected_Quantile_Minneapolis_2021-2024.csv ---\n",
      "\n",
      "Save complete. The file 'NASA_Corrected_Quantile_Minneapolis_2021-2024.csv' is ready for evaluation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# --- 1. CONFIGURATION: Define All File Paths and Settings ---\n",
    "print(\"--- Initializing Configuration for Quantile Mapping ---\")\n",
    "ROOT_DATA_DIR = r\"C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\" # IMPORTANT: Use your actual path\n",
    "\n",
    "# Target coordinates for Minneapolis\n",
    "TARGET_LAT = 45.125\n",
    "TARGET_LON = 266.625\n",
    "\n",
    "# Input files for calculating the bias (2000-2020)\n",
    "HISTORICAL_NASA_FILE_P1 = os.path.join(ROOT_DATA_DIR, \"NASA_Standardized_Minnesota_2000-2014.csv\")\n",
    "HISTORICAL_NASA_FILE_P2 = os.path.join(ROOT_DATA_DIR, \"NASA_Standardized_Minnesota_2015-2020.csv\")\n",
    "HISTORICAL_ERA5_FILE = os.path.join(ROOT_DATA_DIR, \"ERA5_Train_2000-2020.csv\")\n",
    "\n",
    "# Input file to be corrected (2021-2024)\n",
    "VALIDATION_NASA_FILE = os.path.join(ROOT_DATA_DIR, \"NASA_Standardized_Minnesota_2021-2024.csv\")\n",
    "\n",
    "# Final output file for this method\n",
    "OUTPUT_CORRECTED_FILE = os.path.join(ROOT_DATA_DIR, \"NASA_Corrected_Quantile_Minneapolis_2021-2024.csv\")\n",
    "\n",
    "# Define the full calibration period\n",
    "CALIBRATION_START = '2000-01-01'\n",
    "CALIBRATION_END = '2020-12-31'\n",
    "\n",
    "\n",
    "# --- 2. DATA LOADING AND FILTERING ---\n",
    "print(\"\\n--- Step 2: Loading and Filtering all necessary data files for Minneapolis ---\")\n",
    "\n",
    "def load_and_filter_nasa(file_path, lat, lon):\n",
    "    df = pd.read_csv(file_path, parse_dates=['time'])\n",
    "    filtered_df = df[(np.isclose(df['lat'], lat)) & (np.isclose(df['lon'], lon))].copy()\n",
    "    return filtered_df.drop(columns=['lat', 'lon']).set_index('time')\n",
    "\n",
    "nasa_hist_p1_df = load_and_filter_nasa(HISTORICAL_NASA_FILE_P1, TARGET_LAT, TARGET_LON)\n",
    "nasa_hist_p2_df = load_and_filter_nasa(HISTORICAL_NASA_FILE_P2, TARGET_LAT, TARGET_LON)\n",
    "nasa_hist_df = pd.concat([nasa_hist_p1_df, nasa_hist_p2_df])\n",
    "print(f\"  Successfully loaded and filtered historical NASA data for Minneapolis ({nasa_hist_df.index.min().year}-{nasa_hist_df.index.max().year}).\")\n",
    "\n",
    "nasa_val_df = load_and_filter_nasa(VALIDATION_NASA_FILE, TARGET_LAT, TARGET_LON)\n",
    "print(\"  Successfully loaded and filtered validation NASA data for Minneapolis.\")\n",
    "\n",
    "era5_hist_df_hourly = pd.read_csv(HISTORICAL_ERA5_FILE, parse_dates=['time']).set_index('time')\n",
    "print(\"  All data loading complete.\")\n",
    "\n",
    "\n",
    "# --- 3. PREPARE CALIBRATION DATA ---\n",
    "print(f\"\\n--- Step 3: Preparing daily summaries for the calibration period ({CALIBRATION_START} to {CALIBRATION_END}) ---\")\n",
    "\n",
    "era5_daily_summary = era5_hist_df_hourly.resample('D').agg(\n",
    "    air_temperature_k=('air_temperature_k', 'mean'),\n",
    "    tasmin=('air_temperature_k', 'min'),\n",
    "    tasmax=('air_temperature_k', 'max'),\n",
    "    wind_speed_ms=('wind_speed_ms', 'mean'),\n",
    "    relative_humidity_percent=('relative_humidity_percent', 'mean'),\n",
    "    solar_radiation_w_m2=('solar_radiation_w_m2', 'mean'),\n",
    "    thermal_radiation_w_m2=('thermal_radiation_w_m2', 'mean'),\n",
    "    precip_hourly_mm=('precip_hourly_mm', 'sum')\n",
    ").loc[CALIBRATION_START:CALIBRATION_END]\n",
    "\n",
    "COLUMN_MAP = {\n",
    "    'tas': 'air_temperature_k',\n",
    "    'sfcWind': 'wind_speed_ms',\n",
    "    'hurs': 'relative_humidity_percent',\n",
    "    'rsds': 'solar_radiation_w_m2',\n",
    "    'rlds': 'thermal_radiation_w_m2',\n",
    "    'precip_daily_mm': 'precip_hourly_mm'\n",
    "}\n",
    "nasa_hist_df_renamed = nasa_hist_df.rename(columns=COLUMN_MAP)\n",
    "print(\"  Daily summaries created and columns aligned.\")\n",
    "\n",
    "\n",
    "# --- 4. PERFORM QUANTILE MAPPING ---\n",
    "print(\"\\n--- Step 4: Applying Quantile Mapping to validation data ---\")\n",
    "nasa_corrected_df = nasa_val_df.copy()\n",
    "\n",
    "# List of original NASA column names to be corrected\n",
    "VARS_TO_CORRECT = ['tas', 'tasmin', 'tasmax', 'sfcWind', 'hurs', 'rsds', 'rlds', 'precip_daily_mm']\n",
    "\n",
    "for nasa_col in VARS_TO_CORRECT:\n",
    "    # Find the corresponding column name in the ERA5/renamed NASA data\n",
    "    # For tasmin/tasmax, the name is the same. For others, use the map.\n",
    "    era5_col = COLUMN_MAP.get(nasa_col, nasa_col)\n",
    "    \n",
    "    print(f\"  Processing variable: '{nasa_col}' -> '{era5_col}'\")\n",
    "    \n",
    "    # Isolate the historical data for the current variable\n",
    "    source_data = nasa_hist_df_renamed[[era5_col]].dropna()\n",
    "    target_data = era5_daily_summary[[era5_col]].dropna()\n",
    "    \n",
    "    # Isolate the validation data for the current variable\n",
    "    validation_data = nasa_val_df[[nasa_col]].dropna()\n",
    "\n",
    "    # Reshape data for scikit-learn (expects a 2D array)\n",
    "    source_reshaped = source_data.values.reshape(-1, 1)\n",
    "    target_reshaped = target_data.values.reshape(-1, 1)\n",
    "    validation_reshaped = validation_data.values.reshape(-1, 1)\n",
    "\n",
    "    # Learn the distributions of the source (NASA) and target (ERA5) data\n",
    "    # n_quantiles determines the resolution of the mapping. 1000 is a good default.\n",
    "    source_qt = QuantileTransformer(output_distribution='uniform', n_quantiles=1000).fit(source_reshaped)\n",
    "    target_qt = QuantileTransformer(output_distribution='uniform', n_quantiles=1000).fit(target_reshaped)\n",
    "\n",
    "    # Apply the mapping\n",
    "    # 1. Transform the validation data to its quantile values using the source (NASA) transformer\n",
    "    quantiles = source_qt.transform(validation_reshaped)\n",
    "    # 2. Inverse-transform the quantiles using the target (ERA5) transformer to get corrected values\n",
    "    corrected_values = target_qt.inverse_transform(quantiles)\n",
    "\n",
    "    # Place the corrected values back into our results DataFrame\n",
    "    nasa_corrected_df.loc[validation_data.index, nasa_col] = corrected_values.flatten()\n",
    "\n",
    "print(\"  Quantile mapping applied successfully.\")\n",
    "\n",
    "\n",
    "# --- 5. SAVE THE CORRECTED FILE ---\n",
    "print(f\"\\n--- Step 5: Saving bias-corrected data to {OUTPUT_CORRECTED_FILE} ---\")\n",
    "nasa_corrected_df.to_csv(OUTPUT_CORRECTED_FILE)\n",
    "print(f\"\\nSave complete. The file '{os.path.basename(OUTPUT_CORRECTED_FILE)}' is ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dda797a-fcf8-45b8-a6e4-cb4d7909a51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Configuring file paths for QUANTILE MAPPING evaluation ---\n",
      "\n",
      "--- Step 2: Loading data and pre-trained models ---\n",
      "  Successfully loaded bias-corrected (Quantile Method) NASA daily data.\n",
      "  Successfully loaded ERA5 hourly ground truth data.\n",
      "  Loading pre-trained models (trained on 2000-2018 ERA5)...\n",
      "  All models loaded successfully.\n",
      "\n",
      "--- Step 3: Preparing bias-corrected NASA data for prediction ---\n",
      "  Applying Stage-1 models to generate wind characteristics...\n",
      "  Predictor data prepared.\n",
      "\n",
      "--- Step 4: Generating hourly predictions from corrected daily data ---\n",
      "  Predicting hourly values for: air_temperature_k...\n",
      "  Predicting hourly values for: wind_speed_ms...\n",
      "  Predicting hourly values for: relative_humidity_percent...\n",
      "--- Hourly predictions generated successfully. ---\n",
      "\n",
      "--- Step 5: Validating predictions against ERA5 ground truth ---\n",
      "\n",
      "  Final Validation Results (QUANTILE MAPPING METHOD vs. ERA5 Actuals for 2021-2024):\n",
      "    - air_temperature_k:\n",
      "        Mean Absolute Error (MAE):    7.2550\n",
      "        Root Mean Squared Error (RMSE): 9.1003\n",
      "        R-squared (R²):               0.4997\n",
      "    - wind_speed_ms:\n",
      "        Mean Absolute Error (MAE):    1.6080\n",
      "        Root Mean Squared Error (RMSE): 2.0460\n",
      "        R-squared (R²):               -0.5851\n",
      "    - relative_humidity_percent:\n",
      "        Mean Absolute Error (MAE):    20.8362\n",
      "        Root Mean Squared Error (RMSE): 25.3465\n",
      "        R-squared (R²):               -1.1526\n",
      "\n",
      "--- Step 6: Saving final validation results to C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\\QUANTILE_METHOD_VALIDATION_NASA_vs_ERA5_2021-2024.csv ---\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "print(\"--- Step 1: Configuring file paths for QUANTILE MAPPING evaluation ---\")\n",
    "ROOT_DATA_DIR = r\"C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\" # IMPORTANT: Use your actual path\n",
    "\n",
    "# *** CHANGED ***: Use the Quantile-Corrected file as the input\n",
    "NASA_DAILY_INPUT_FILE = os.path.join(ROOT_DATA_DIR, \"NASA_Corrected_Quantile_Minneapolis_2021-2024.csv\")\n",
    "\n",
    "# Other required files\n",
    "ERA5_HOURLY_GROUND_TRUTH_FILE = os.path.join(ROOT_DATA_DIR, \"ERA5_Test_2021-2024.csv\")\n",
    "MODEL_SAVE_DIR = os.path.join(ROOT_DATA_DIR, \"trained_models_temporal_holdout\")\n",
    "\n",
    "# *** CHANGED ***: Define a new output file for this validation run\n",
    "FINAL_VALIDATION_OUTPUT_FILE = os.path.join(ROOT_DATA_DIR, \"QUANTILE_METHOD_VALIDATION_NASA_vs_ERA5_2021-2024.csv\")\n",
    "\n",
    "\n",
    "# --- 2. LOAD DATA AND TRAINED MODELS ---\n",
    "print(\"\\n--- Step 2: Loading data and pre-trained models ---\")\n",
    "\n",
    "# Load the bias-corrected daily NASA data\n",
    "try:\n",
    "    nasa_df = pd.read_csv(NASA_DAILY_INPUT_FILE, parse_dates=['time']).set_index('time')\n",
    "    print(f\"  Successfully loaded bias-corrected (Quantile Method) NASA daily data.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: NASA data file not found at {NASA_DAILY_INPUT_FILE}.\")\n",
    "    print(\"Please ensure you have run the Quantile Mapping script first.\")\n",
    "    exit()\n",
    "\n",
    "# Load the hourly ERA5 ground truth data\n",
    "try:\n",
    "    era5_df = pd.read_csv(ERA5_HOURLY_GROUND_TRUTH_FILE, index_col='time', parse_dates=True)\n",
    "    print(\"  Successfully loaded ERA5 hourly ground truth data.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: ERA5 ground truth file not found at {ERA5_HOURLY_GROUND_TRUTH_FILE}.\")\n",
    "    exit()\n",
    "\n",
    "# Load the library of trained models\n",
    "try:\n",
    "    print(\"  Loading pre-trained models (trained on 2000-2018 ERA5)...\")\n",
    "    predictor_map = {\n",
    "        'air_temperature_k': ['air_temperature_k_mean', 'air_temperature_k_min', 'air_temperature_k_max'],\n",
    "        'wind_speed_ms': ['wind_speed_ms_mean', 'wind_speed_ms_max', 'wind_speed_ms_std'],\n",
    "        'relative_humidity_percent': ['relative_humidity_percent_mean']\n",
    "    }\n",
    "    trained_models = {}\n",
    "    for var_name in predictor_map.keys():\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'models_{var_name}.pkl')\n",
    "        trained_models[var_name] = joblib.load(model_path)\n",
    "    \n",
    "    trained_models['wind_max_from_mean'] = joblib.load(os.path.join(MODEL_SAVE_DIR, 'model_wind_max.pkl'))\n",
    "    trained_models['wind_std_from_mean'] = joblib.load(os.path.join(MODEL_SAVE_DIR, 'model_wind_std.pkl'))\n",
    "    print(\"  All models loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading model file: {e.filename}. Please ensure training was successful.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 3. PREPARE NASA DATA FOR PREDICTION ---\n",
    "print(\"\\n--- Step 3: Preparing bias-corrected NASA data for prediction ---\")\n",
    "\n",
    "# Rename the NASA columns to match the names the models were trained on\n",
    "nasa_predictors_df = nasa_df.rename(columns={\n",
    "    'tas': 'air_temperature_k_mean',\n",
    "    'tasmin': 'air_temperature_k_min',\n",
    "    'tasmax': 'air_temperature_k_max',\n",
    "    'sfcWind': 'wind_speed_ms_mean',\n",
    "    'hurs': 'relative_humidity_percent_mean'\n",
    "})\n",
    "\n",
    "# Apply the two-stage model to generate wind characteristics\n",
    "print(\"  Applying Stage-1 models to generate wind characteristics...\")\n",
    "X_wind_mean = nasa_predictors_df[['wind_speed_ms_mean']]\n",
    "nasa_predictors_df['wind_speed_ms_max'] = trained_models['wind_max_from_mean'].predict(X_wind_mean)\n",
    "nasa_predictors_df['wind_speed_ms_std'] = trained_models['wind_std_from_mean'].predict(X_wind_mean)\n",
    "print(\"  Predictor data prepared.\")\n",
    "\n",
    "\n",
    "# --- 4. GENERATE HOURLY PREDICTIONS ---\n",
    "print(\"\\n--- Step 4: Generating hourly predictions from corrected daily data ---\")\n",
    "final_predictions = {}\n",
    "for var_name, predictors in predictor_map.items():\n",
    "    print(f\"  Predicting hourly values for: {var_name}...\")\n",
    "    hourly_preds_list = []\n",
    "    if not all(p in nasa_predictors_df.columns for p in predictors):\n",
    "        print(f\"    Skipping {var_name}, missing one or more predictors: {predictors}\")\n",
    "        continue\n",
    "        \n",
    "    X_predict = nasa_predictors_df[predictors]\n",
    "    for hour in range(24):\n",
    "        model = trained_models[var_name].get(hour)\n",
    "        if model:\n",
    "            preds = model.predict(X_predict)\n",
    "            hourly_preds_list.append(pd.Series(preds, index=X_predict.index, name=hour))\n",
    "            \n",
    "    if hourly_preds_list:\n",
    "        var_df_wide = pd.concat(hourly_preds_list, axis=1)\n",
    "        var_stacked = var_df_wide.stack()\n",
    "        var_stacked.index = var_stacked.index.map(lambda x: x[0] + pd.to_timedelta(x[1], unit='h'))\n",
    "        final_predictions[f'predicted_{var_name}'] = var_stacked\n",
    "\n",
    "predictions_df = pd.DataFrame(final_predictions)\n",
    "print(\"--- Hourly predictions generated successfully. ---\")\n",
    "\n",
    "\n",
    "# --- 5. VALIDATE PREDICTIONS AGAINST ERA5 GROUND TRUTH ---\n",
    "print(\"\\n--- Step 5: Validating predictions against ERA5 ground truth ---\")\n",
    "validation_df = pd.merge(\n",
    "    era5_df.rename(columns=lambda c: f\"actual_{c}\"),\n",
    "    predictions_df,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# *** CHANGED ***: Updated the title of the results printout\n",
    "print(\"\\n  Final Validation Results (QUANTILE MAPPING METHOD vs. ERA5 Actuals for 2021-2024):\")\n",
    "for var_name in predictor_map.keys():\n",
    "    actual_col = f'actual_{var_name}'\n",
    "    predicted_col = f'predicted_{var_name}'\n",
    "    if actual_col in validation_df.columns and predicted_col in validation_df.columns:\n",
    "        temp_compare_df = validation_df[[actual_col, predicted_col]].dropna()\n",
    "        if not temp_compare_df.empty:\n",
    "            mae = mean_absolute_error(temp_compare_df[actual_col], temp_compare_df[predicted_col])\n",
    "            rmse = np.sqrt(mean_squared_error(temp_compare_df[actual_col], temp_compare_df[predicted_col]))\n",
    "            r2 = r2_score(temp_compare_df[actual_col], temp_compare_df[predicted_col])\n",
    "            \n",
    "            print(f\"    - {var_name}:\")\n",
    "            print(f\"        Mean Absolute Error (MAE):    {mae:.4f}\")\n",
    "            print(f\"        Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "            print(f\"        R-squared (R²):               {r2:.4f}\")\n",
    "\n",
    "# --- 6. SAVE THE FINAL VALIDATION RESULTS ---\n",
    "print(f\"\\n--- Step 6: Saving final validation results to {FINAL_VALIDATION_OUTPUT_FILE} ---\")\n",
    "validation_df.to_csv(FINAL_VALIDATION_OUTPUT_FILE)\n",
    "print(\"Save complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d29a3-a8eb-4deb-907e-63f275a2198c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

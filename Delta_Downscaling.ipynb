{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6056948b-2384-48be-9168-4fdd93ea2250",
   "metadata": {},
   "source": [
    "# Climate Data Bias Correction & Downscaling Workflow\n",
    "\n",
    "This plan outlines a structured approach to improving the accuracy of hourly climate predictions using bias correction and downscaling methods.\n",
    "\n",
    "#### Phase 1: Data Preparation & Bias Correction (**Current Phase**)\n",
    "\n",
    "> **Goal**: Create two corrected daily NASA datasets (2021-2024) using bias correction techniques, based on historical calibration data (2000-2014).\n",
    "\n",
    "\n",
    "##### Task 1.1: Consolidate Historical Data\n",
    "\n",
    "- **NASA**:  \n",
    "  - Load and process `NASA_Standardized_Minnesota_2000-2014.csv`\n",
    "\n",
    "- **ERA5**:  \n",
    "  - From `ERA5_Train_2000-2020.csv`, extract daily summaries for 2000-2014 to align with NASA data.\n",
    "\n",
    "##### Task 1.2: Implement Workflow A – *Delta Change Method*\n",
    "\n",
    "- **Method**:  \n",
    "  - Calculate historical monthly *differences* (for temperature) or *ratios* (for wind/precipitation) between ERA5 and NASA data (2000-2014).\n",
    "  - Apply these deltas to raw NASA data from 2021-2024.\n",
    "\n",
    "- **Output**:  \n",
    "  - `NASA_Corrected_Delta_2021-2024.csv`\n",
    "\n",
    "##### Task 1.3: Implement Workflow B – *Quantile Mapping Method*\n",
    "\n",
    "- **Method**:  \n",
    "  - Build a quantile mapping function per variable (temperature, wind, etc.) to align NASA data distributions with ERA5 (2000-2014).\n",
    "  - Apply the mapping functions to the 2021-2024 NASA data.\n",
    "\n",
    "- **Output**:  \n",
    "  - `NASA_Corrected_Quantile_2021-2024.csv`\n",
    "\n",
    "---\n",
    "\n",
    "#### Phase 2: Downscaling & Prediction\n",
    "\n",
    "> **Goal**: Test whether the corrected input data improves performance using existing trained models.\n",
    "\n",
    "##### Task 2.1: Run Downscaling on Corrected Data\n",
    "\n",
    "- Use the script `DownScaling.ipynb` and run it twice:\n",
    "  - **Run 1**: Use `NASA_Corrected_Delta_2021-2024.csv`\n",
    "  - **Run 2**: Use `NASA_Corrected_Quantile_2021-2024.csv`\n",
    "\n",
    "- **Output**:  \n",
    "  - Two sets of hourly predictions for 2021-2024.\n",
    "\n",
    "\n",
    "#### Phase 3: Evaluation & Decision\n",
    "\n",
    "> **Goal**: Evaluate model performance and decide the best correction method.\n",
    "\n",
    "\n",
    "##### Task 3.1: Compare Against Ground Truth\n",
    "\n",
    "- Compare both sets of hourly predictions with actual hourly ERA5 data from `ERA5_Test_2021-2024.csv`.\n",
    "- Calculate standard metrics:\n",
    "  - **MAE**\n",
    "  - **RMSE**\n",
    "  - **R²**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a01522-c940-43af-966f-8a992c24cd7c",
   "metadata": {},
   "source": [
    "# Bias Correction with the Delta Change Method\n",
    "\n",
    "## Context & The Problem\n",
    "\n",
    "Our initial downscaling model, though structurally robust, produced **hourly predictions with significant errors** — particularly high **Mean Absolute Error (MAE)** and **Root Mean Squared Error (RMSE)** — when compared against **ERA5 ground-truth data**.\n",
    "\n",
    "This discrepancy revealed a **systematic bias** between:\n",
    "\n",
    "- The **daily outputs of the NASA climate model**, and\n",
    "- The **daily summaries derived from ERA5 observations**\n",
    "\n",
    "> To improve the accuracy of the final **hourly downscaled predictions**, we must **first correct the daily input data**.\n",
    "\n",
    "\n",
    "## Proposed Solution: The Delta Change Method\n",
    "\n",
    "The **Delta Change Method** is one of the most widely used and effective **bias correction techniques** in climate data analysis. It applies historical bias adjustments to future model outputs in a **month-by-month** manner.\n",
    "\n",
    "\n",
    "### What is the Delta Change Method?\n",
    "\n",
    "The method assumes that the **average model error** (or \"delta\") for a given calendar month, observed during a **historical calibration period**, will remain **consistent in the future**.\n",
    "\n",
    "Instead of trying to re-learn a complex statistical relationship, this method simply **corrects future projections** using **historical monthly averages** of model bias.\n",
    "\n",
    "\n",
    "### How Does It Work?\n",
    "\n",
    "The Delta Method is applied in two steps:\n",
    "\n",
    "#### 1. **Learn (Calibration Phase)**\n",
    "\n",
    "- Use a historical period (e.g., **2000–2020**)  \n",
    "- For each calendar month, compute the **bias between NASA and ERA5** data:\n",
    "  - **Additive correction** for variables like temperature  \n",
    "    (e.g., NASA_temp - ERA5_temp)\n",
    "  - **Multiplicative correction** for variables like wind speed  \n",
    "    (e.g., NASA_wind / ERA5_wind)\n",
    "\n",
    "#### 2. **Correct (Application Phase)**\n",
    "\n",
    "- Apply the monthly delta to future model outputs (e.g., **2021–2024**)\n",
    "- Example corrections:\n",
    "  - If January temperatures were historically **+2°C too warm**, subtract 2°C from all **future January temperatures**\n",
    "  - If July wind speeds were **10% too high**, multiply all future July wind speeds by **0.90**\n",
    "\n",
    ">  This correction brings the **statistical properties of future NASA data** more in line with the **ERA5 observations**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Goal of This Notebook\n",
    "\n",
    "The purpose of this notebook is to:\n",
    "\n",
    "- Apply the **Delta Change Method** to the **daily NASA input data**\n",
    "- Use the corrected daily data as input to our **hourly downscaling models**\n",
    "- Evaluate whether this bias correction step improves the accuracy of the final **hourly predictions** when compared to results from uncorrected data\n",
    "\n",
    "> This process is essential for ensuring that the **downscaled data maintains physical realism and statistical alignment** with the real-world observations it is meant to replicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a2814-e8f3-4dab-8312-ad636675a49d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "101b761f-c9d3-4b77-8074-7c64e88d6968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Configuration ---\n",
      "\n",
      "--- Step 2: Loading and Filtering all necessary data files for Minneapolis ---\n",
      "  Successfully loaded and filtered historical NASA data for Minneapolis (2000-2020).\n",
      "  Successfully loaded and filtered validation NASA data for Minneapolis.\n",
      "  All data loading complete.\n",
      "\n",
      "--- Step 3: Preparing daily summaries for the calibration period (2000-01-01 to 2020-12-31) ---\n",
      "  Daily summaries created and columns aligned.\n",
      "\n",
      "--- Step 4: Calculating historical monthly deltas based on 2000-2020 data ---\n",
      "  Monthly deltas calculated successfully:\n",
      "       air_temperature_k    tasmin    tasmax  wind_speed_ms  \\\n",
      "month                                                         \n",
      "1              -0.900845  0.578580 -1.459750       0.965596   \n",
      "2              -0.262087  0.899105 -0.363040       0.980524   \n",
      "3               1.198082  2.416696  1.031728       0.979971   \n",
      "4              -0.697369  0.822795 -0.945179       1.022300   \n",
      "5              -0.214768  1.407489 -0.772635       1.056851   \n",
      "6               0.413619  2.300134 -0.639085       1.180852   \n",
      "7               0.087925  1.650367 -0.830065       1.142579   \n",
      "8              -1.007459  0.814121 -2.139577       1.121366   \n",
      "9              -0.397149  1.514042 -1.169272       1.155131   \n",
      "10             -1.231939  0.893882 -2.177939       1.056434   \n",
      "11             -0.327226  1.216042 -0.831199       1.040448   \n",
      "12             -0.133278  1.346929 -0.955937       0.960307   \n",
      "\n",
      "       relative_humidity_percent  solar_radiation_w_m2  \\\n",
      "month                                                    \n",
      "1                       0.888775              0.000422   \n",
      "2                       0.862466              0.000413   \n",
      "3                       0.814872              0.000381   \n",
      "4                       0.747606              0.000348   \n",
      "5                       0.765084              0.000327   \n",
      "6                       0.816433              0.000323   \n",
      "7                       0.872609              0.000342   \n",
      "8                       0.981831              0.000351   \n",
      "9                       1.001334              0.000369   \n",
      "10                      0.960081              0.000376   \n",
      "11                      0.899982              0.000400   \n",
      "12                      0.922567              0.000397   \n",
      "\n",
      "       thermal_radiation_w_m2  precip_hourly_mm  \n",
      "month                                            \n",
      "1                    0.000294          0.095450  \n",
      "2                    0.000293          0.133048  \n",
      "3                    0.000296          0.093750  \n",
      "4                    0.000285          0.113084  \n",
      "5                    0.000286          0.102541  \n",
      "6                    0.000288          0.081819  \n",
      "7                    0.000283          0.079134  \n",
      "8                    0.000279          0.092421  \n",
      "9                    0.000287          0.099043  \n",
      "10                   0.000290          0.113102  \n",
      "11                   0.000300          0.114033  \n",
      "12                   0.000304          0.153664  \n",
      "\n",
      "--- Step 5: Applying deltas to the Minneapolis validation dataset ---\n",
      "  Deltas applied successfully.\n",
      "\n",
      "--- Step 6: Saving bias-corrected data to C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\\NASA_Corrected_Delta_Minneapolis_2021-2024.csv ---\n",
      "\n",
      "Save complete. The file 'NASA_Corrected_Delta_Minneapolis_2021-2024.csv' is ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. CONFIGURATION: Define All File Paths and Settings ---\n",
    "print(\"--- Initializing Configuration ---\")\n",
    "ROOT_DATA_DIR = r\"C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\" # IMPORTANT: Use your actual path\n",
    "\n",
    "# Target coordinates for Minneapolis\n",
    "TARGET_LAT = 45.125\n",
    "TARGET_LON = 266.625\n",
    "\n",
    "# Input files for calculating the bias (2000-2020)\n",
    "HISTORICAL_NASA_FILE_P1 = os.path.join(ROOT_DATA_DIR, \"NASA_Standardized_Minnesota_2000-2014.csv\")\n",
    "HISTORICAL_NASA_FILE_P2 = os.path.join(ROOT_DATA_DIR, \"NASA_Standardized_Minnesota_2015-2020.csv\")\n",
    "HISTORICAL_ERA5_FILE = os.path.join(ROOT_DATA_DIR, \"ERA5_Train_2000-2020.csv\")\n",
    "\n",
    "# Input file to be corrected (2021-2024)\n",
    "VALIDATION_NASA_FILE = os.path.join(ROOT_DATA_DIR, \"NASA_Standardized_Minnesota_2021-2024.csv\")\n",
    "\n",
    "# Final output file\n",
    "OUTPUT_CORRECTED_FILE = os.path.join(ROOT_DATA_DIR, \"NASA_Corrected_Delta_Minneapolis_2021-2024.csv\") # Renamed output for clarity\n",
    "\n",
    "# Define the full calibration period\n",
    "CALIBRATION_START = '2000-01-01'\n",
    "CALIBRATION_END = '2020-12-31'\n",
    "\n",
    "\n",
    "# --- 2. DATA LOADING AND FILTERING ---\n",
    "print(\"\\n--- Step 2: Loading and Filtering all necessary data files for Minneapolis ---\")\n",
    "\n",
    "# Define a helper function to load and filter NASA data\n",
    "def load_and_filter_nasa(file_path, lat, lon):\n",
    "    df = pd.read_csv(file_path, parse_dates=['time'])\n",
    "    # Filter for the specific grid point\n",
    "    filtered_df = df[\n",
    "        (np.isclose(df['lat'], lat)) &\n",
    "        (np.isclose(df['lon'], lon))\n",
    "    ].copy()\n",
    "    # Drop the now-redundant lat/lon columns and set time index\n",
    "    return filtered_df.drop(columns=['lat', 'lon']).set_index('time')\n",
    "\n",
    "# Load and filter historical NASA data\n",
    "nasa_hist_p1_df = load_and_filter_nasa(HISTORICAL_NASA_FILE_P1, TARGET_LAT, TARGET_LON)\n",
    "nasa_hist_p2_df = load_and_filter_nasa(HISTORICAL_NASA_FILE_P2, TARGET_LAT, TARGET_LON)\n",
    "nasa_hist_df = pd.concat([nasa_hist_p1_df, nasa_hist_p2_df])\n",
    "print(f\"  Successfully loaded and filtered historical NASA data for Minneapolis ({nasa_hist_df.index.min().year}-{nasa_hist_df.index.max().year}).\")\n",
    "\n",
    "# Load and filter validation NASA data\n",
    "nasa_val_df = load_and_filter_nasa(VALIDATION_NASA_FILE, TARGET_LAT, TARGET_LON)\n",
    "print(\"  Successfully loaded and filtered validation NASA data for Minneapolis.\")\n",
    "\n",
    "# Load the ERA5 file (it's already for Minneapolis)\n",
    "era5_hist_df_hourly = pd.read_csv(HISTORICAL_ERA5_FILE, parse_dates=['time']).set_index('time')\n",
    "print(\"  All data loading complete.\")\n",
    "\n",
    "\n",
    "# --- 3. PREPARE CALIBRATION DATA ---\n",
    "print(f\"\\n--- Step 3: Preparing daily summaries for the calibration period ({CALIBRATION_START} to {CALIBRATION_END}) ---\")\n",
    "\n",
    "# Resample ERA5 hourly data to daily summaries\n",
    "era5_daily_summary = era5_hist_df_hourly.resample('D').agg(\n",
    "    air_temperature_k=('air_temperature_k', 'mean'),\n",
    "    tasmin=('air_temperature_k', 'min'),\n",
    "    tasmax=('air_temperature_k', 'max'),\n",
    "    wind_speed_ms=('wind_speed_ms', 'mean'),\n",
    "    relative_humidity_percent=('relative_humidity_percent', 'mean'),\n",
    "    solar_radiation_w_m2=('solar_radiation_w_m2', 'mean'),\n",
    "    thermal_radiation_w_m2=('thermal_radiation_w_m2', 'mean'),\n",
    "    precip_hourly_mm=('precip_hourly_mm', 'sum')\n",
    ").loc[CALIBRATION_START:CALIBRATION_END]\n",
    "\n",
    "# Create a mapping to align all NASA column names with the ERA5 summary column names\n",
    "COLUMN_MAP = {\n",
    "    'tas': 'air_temperature_k',\n",
    "    'sfcWind': 'wind_speed_ms',\n",
    "    'hurs': 'relative_humidity_percent',\n",
    "    'rsds': 'solar_radiation_w_m2',\n",
    "    'rlds': 'thermal_radiation_w_m2',\n",
    "    'precip_daily_mm': 'precip_hourly_mm'\n",
    "}\n",
    "nasa_hist_df_renamed = nasa_hist_df.rename(columns=COLUMN_MAP)\n",
    "print(\"  Daily summaries created and columns aligned.\")\n",
    "\n",
    "\n",
    "# --- 4. CALCULATE MONTHLY DELTAS ---\n",
    "print(\"\\n--- Step 4: Calculating historical monthly deltas based on 2000-2020 data ---\")\n",
    "\n",
    "mean_monthly_era5 = era5_daily_summary.groupby(era5_daily_summary.index.month).mean()\n",
    "mean_monthly_nasa = nasa_hist_df_renamed.groupby(nasa_hist_df_renamed.index.month).mean()\n",
    "\n",
    "ADDITIVE_VARS = ['air_temperature_k', 'tasmin', 'tasmax']\n",
    "MULTIPLICATIVE_VARS = ['wind_speed_ms', 'relative_humidity_percent', 'solar_radiation_w_m2', 'thermal_radiation_w_m2', 'precip_hourly_mm']\n",
    "\n",
    "additive_deltas = mean_monthly_era5[ADDITIVE_VARS] - mean_monthly_nasa[ADDITIVE_VARS]\n",
    "multiplicative_deltas = mean_monthly_era5[MULTIPLICATIVE_VARS] / mean_monthly_nasa[MULTIPLICATIVE_VARS]\n",
    "multiplicative_deltas = multiplicative_deltas.replace([np.inf, -np.inf], 1).fillna(1)\n",
    "\n",
    "deltas_df = pd.concat([additive_deltas, multiplicative_deltas], axis=1)\n",
    "deltas_df.index.name = 'month'\n",
    "print(\"  Monthly deltas calculated successfully:\")\n",
    "print(deltas_df)\n",
    "\n",
    "\n",
    "# --- 5. APPLY DELTAS TO THE VALIDATION DATASET ---\n",
    "print(f\"\\n--- Step 5: Applying deltas to the Minneapolis validation dataset ---\")\n",
    "nasa_corrected_df = nasa_val_df.copy()\n",
    "nasa_corrected_df['month'] = nasa_corrected_df.index.month\n",
    "\n",
    "for var in ADDITIVE_VARS:\n",
    "    nasa_corrected_df[var + '_delta'] = nasa_corrected_df['month'].map(deltas_df[var])\n",
    "for var in MULTIPLICATIVE_VARS:\n",
    "    nasa_corrected_df[var + '_delta'] = nasa_corrected_df['month'].map(deltas_df[var])\n",
    "\n",
    "nasa_corrected_df['tas'] = nasa_corrected_df['tas'] + nasa_corrected_df['air_temperature_k_delta']\n",
    "nasa_corrected_df['tasmin'] = nasa_corrected_df['tasmin'] + nasa_corrected_df['tasmin_delta']\n",
    "nasa_corrected_df['tasmax'] = nasa_corrected_df['tasmax'] + nasa_corrected_df['tasmax_delta']\n",
    "nasa_corrected_df['sfcWind'] = nasa_corrected_df['sfcWind'] * nasa_corrected_df['wind_speed_ms_delta']\n",
    "nasa_corrected_df['hurs'] = nasa_corrected_df['hurs'] * nasa_corrected_df['relative_humidity_percent_delta']\n",
    "nasa_corrected_df['rsds'] = nasa_corrected_df['rsds'] * nasa_corrected_df['solar_radiation_w_m2_delta']\n",
    "nasa_corrected_df['rlds'] = nasa_corrected_df['rlds'] * nasa_corrected_df['thermal_radiation_w_m2_delta']\n",
    "nasa_corrected_df['precip_daily_mm'] = nasa_corrected_df['precip_daily_mm'] * nasa_corrected_df['precip_hourly_mm_delta']\n",
    "\n",
    "nasa_corrected_df['hurs'] = nasa_corrected_df['hurs'].clip(0, 100)\n",
    "nasa_corrected_df = nasa_corrected_df[nasa_val_df.columns]\n",
    "print(\"  Deltas applied successfully.\")\n",
    "\n",
    "\n",
    "# --- 6. SAVE THE CORRECTED FILE ---\n",
    "print(f\"\\n--- Step 6: Saving bias-corrected data to {OUTPUT_CORRECTED_FILE} ---\")\n",
    "nasa_corrected_df.to_csv(OUTPUT_CORRECTED_FILE)\n",
    "print(f\"\\nSave complete. The file '{os.path.basename(OUTPUT_CORRECTED_FILE)}' is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1931d-937d-45d4-a1dd-33184d75f576",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95cb0710-77d3-4ced-bb6c-b1deb5596e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Configuring file paths for evaluation ---\n",
      "\n",
      "--- Step 2: Loading data and pre-trained models ---\n",
      "  Successfully loaded bias-corrected NASA daily data for Minneapolis.\n",
      "  Successfully loaded ERA5 hourly ground truth data.\n",
      "  Loading pre-trained models (trained on 2000-2018 ERA5)...\n",
      "  All models loaded successfully.\n",
      "\n",
      "--- Step 3: Preparing bias-corrected NASA data for prediction ---\n",
      "  Applying Stage-1 models to generate wind characteristics...\n",
      "  Predictor data prepared.\n",
      "\n",
      "--- Step 4: Generating hourly predictions from corrected daily data ---\n",
      "  Predicting hourly values for: air_temperature_k...\n",
      "  Predicting hourly values for: wind_speed_ms...\n",
      "  Predicting hourly values for: relative_humidity_percent...\n",
      "--- Hourly predictions generated successfully. ---\n",
      "\n",
      "--- Step 5: Validating predictions against ERA5 ground truth ---\n",
      "\n",
      "  Final Validation Results (DELTA METHOD vs. ERA5 Actuals for 2021-2024):\n",
      "    - air_temperature_k:\n",
      "        Mean Absolute Error (MAE):    7.2459\n",
      "        Root Mean Squared Error (RMSE): 9.0992\n",
      "        R-squared (R²):               0.4998\n",
      "    - wind_speed_ms:\n",
      "        Mean Absolute Error (MAE):    1.7027\n",
      "        Root Mean Squared Error (RMSE): 2.1698\n",
      "        R-squared (R²):               -0.7828\n",
      "    - relative_humidity_percent:\n",
      "        Mean Absolute Error (MAE):    20.1308\n",
      "        Root Mean Squared Error (RMSE): 24.4422\n",
      "        R-squared (R²):               -1.0017\n",
      "\n",
      "--- Step 6: Saving final validation results to C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\\DELTA_METHOD_VALIDATION_NASA_vs_ERA5_2021-2024.csv ---\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "print(\"--- Step 1: Configuring file paths for evaluation ---\")\n",
    "# Use the exact filenames from your directory\n",
    "ROOT_DATA_DIR = r\"C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\" # IMPORTANT: Use your actual path\n",
    "\n",
    "# *** KEY CHANGE: Use the Minneapolis-specific bias-corrected file as the input ***\n",
    "NASA_DAILY_INPUT_FILE = os.path.join(ROOT_DATA_DIR, \"NASA_Corrected_Delta_Minneapolis_2021-2024.csv\")\n",
    "\n",
    "# Other required files\n",
    "ERA5_HOURLY_GROUND_TRUTH_FILE = os.path.join(ROOT_DATA_DIR, \"ERA5_Test_2021-2024.csv\")\n",
    "MODEL_SAVE_DIR = os.path.join(ROOT_DATA_DIR, \"trained_models_temporal_holdout\")\n",
    "\n",
    "# Define the output file for this specific validation run\n",
    "FINAL_VALIDATION_OUTPUT_FILE = os.path.join(ROOT_DATA_DIR, \"DELTA_METHOD_VALIDATION_NASA_vs_ERA5_2021-2024.csv\")\n",
    "\n",
    "\n",
    "# --- 2. LOAD DATA AND TRAINED MODELS ---\n",
    "print(\"\\n--- Step 2: Loading data and pre-trained models ---\")\n",
    "\n",
    "# Load the bias-corrected daily NASA data\n",
    "try:\n",
    "    nasa_df = pd.read_csv(NASA_DAILY_INPUT_FILE, parse_dates=['time']).set_index('time')\n",
    "    print(f\"  Successfully loaded bias-corrected NASA daily data for Minneapolis.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find the input file: {NASA_DAILY_INPUT_FILE}\")\n",
    "    print(\"Please ensure you have run the Delta Change correction script successfully.\")\n",
    "    exit()\n",
    "\n",
    "# Load the hourly ERA5 ground truth data\n",
    "try:\n",
    "    era5_df = pd.read_csv(ERA5_HOURLY_GROUND_TRUTH_FILE, index_col='time', parse_dates=True)\n",
    "    print(\"  Successfully loaded ERA5 hourly ground truth data.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: ERA5 ground truth file not found at {ERA5_HOURLY_GROUND_TRUTH_FILE}.\")\n",
    "    exit()\n",
    "\n",
    "# Load the library of trained models\n",
    "try:\n",
    "    print(\"  Loading pre-trained models (trained on 2000-2018 ERA5)...\")\n",
    "    predictor_map = {\n",
    "        'air_temperature_k': ['air_temperature_k_mean', 'air_temperature_k_min', 'air_temperature_k_max'],\n",
    "        'wind_speed_ms': ['wind_speed_ms_mean', 'wind_speed_ms_max', 'wind_speed_ms_std'],\n",
    "        'relative_humidity_percent': ['relative_humidity_percent_mean']\n",
    "    }\n",
    "    trained_models = {}\n",
    "    for var_name in predictor_map.keys():\n",
    "        model_path = os.path.join(MODEL_SAVE_DIR, f'models_{var_name}.pkl')\n",
    "        trained_models[var_name] = joblib.load(model_path)\n",
    "    \n",
    "    trained_models['wind_max_from_mean'] = joblib.load(os.path.join(MODEL_SAVE_DIR, 'model_wind_max.pkl'))\n",
    "    trained_models['wind_std_from_mean'] = joblib.load(os.path.join(MODEL_SAVE_DIR, 'model_wind_std.pkl'))\n",
    "    print(\"  All models loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading model file: {e.filename}. Please ensure training was successful.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 3. PREPARE NASA DATA FOR PREDICTION ---\n",
    "print(\"\\n--- Step 3: Preparing bias-corrected NASA data for prediction ---\")\n",
    "\n",
    "# Rename the NASA columns to match the names the models were trained on\n",
    "nasa_predictors_df = nasa_df.rename(columns={\n",
    "    'tas': 'air_temperature_k_mean',\n",
    "    'tasmin': 'air_temperature_k_min',\n",
    "    'tasmax': 'air_temperature_k_max',\n",
    "    'sfcWind': 'wind_speed_ms_mean',\n",
    "    'hurs': 'relative_humidity_percent_mean'\n",
    "})\n",
    "\n",
    "# Apply the two-stage model to generate wind characteristics\n",
    "print(\"  Applying Stage-1 models to generate wind characteristics...\")\n",
    "X_wind_mean = nasa_predictors_df[['wind_speed_ms_mean']]\n",
    "nasa_predictors_df['wind_speed_ms_max'] = trained_models['wind_max_from_mean'].predict(X_wind_mean)\n",
    "nasa_predictors_df['wind_speed_ms_std'] = trained_models['wind_std_from_mean'].predict(X_wind_mean)\n",
    "print(\"  Predictor data prepared.\")\n",
    "\n",
    "\n",
    "# --- 4. GENERATE HOURLY PREDICTIONS ---\n",
    "print(\"\\n--- Step 4: Generating hourly predictions from corrected daily data ---\")\n",
    "final_predictions = {}\n",
    "for var_name, predictors in predictor_map.items():\n",
    "    print(f\"  Predicting hourly values for: {var_name}...\")\n",
    "    hourly_preds_list = []\n",
    "    # Ensure all required predictors exist\n",
    "    if not all(p in nasa_predictors_df.columns for p in predictors):\n",
    "        print(f\"    Skipping {var_name}, missing one or more predictors: {predictors}\")\n",
    "        continue\n",
    "        \n",
    "    X_predict = nasa_predictors_df[predictors]\n",
    "    for hour in range(24):\n",
    "        model = trained_models[var_name].get(hour)\n",
    "        if model:\n",
    "            preds = model.predict(X_predict)\n",
    "            hourly_preds_list.append(pd.Series(preds, index=X_predict.index, name=hour))\n",
    "            \n",
    "    if hourly_preds_list:\n",
    "        var_df_wide = pd.concat(hourly_preds_list, axis=1)\n",
    "        var_stacked = var_df_wide.stack()\n",
    "        var_stacked.index = var_stacked.index.map(lambda x: x[0] + pd.to_timedelta(x[1], unit='h'))\n",
    "        final_predictions[f'predicted_{var_name}'] = var_stacked\n",
    "\n",
    "predictions_df = pd.DataFrame(final_predictions)\n",
    "print(\"--- Hourly predictions generated successfully. ---\")\n",
    "\n",
    "\n",
    "# --- 5. VALIDATE PREDICTIONS AGAINST ERA5 GROUND TRUTH ---\n",
    "print(\"\\n--- Step 5: Validating predictions against ERA5 ground truth ---\")\n",
    "# Merge predictions with the actual hourly ERA5 data\n",
    "validation_df = pd.merge(\n",
    "    era5_df.rename(columns=lambda c: f\"actual_{c}\"),\n",
    "    predictions_df,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Calculate and print final error metrics\n",
    "print(\"\\n  Final Validation Results (DELTA METHOD vs. ERA5 Actuals for 2021-2024):\")\n",
    "for var_name in predictor_map.keys():\n",
    "    actual_col = f'actual_{var_name}'\n",
    "    predicted_col = f'predicted_{var_name}'\n",
    "    if actual_col in validation_df.columns and predicted_col in validation_df.columns:\n",
    "        temp_compare_df = validation_df[[actual_col, predicted_col]].dropna()\n",
    "        if not temp_compare_df.empty:\n",
    "            mae = mean_absolute_error(temp_compare_df[actual_col], temp_compare_df[predicted_col])\n",
    "            rmse = np.sqrt(mean_squared_error(temp_compare_df[actual_col], temp_compare_df[predicted_col]))\n",
    "            r2 = r2_score(temp_compare_df[actual_col], temp_compare_df[predicted_col])\n",
    "            \n",
    "            print(f\"    - {var_name}:\")\n",
    "            print(f\"        Mean Absolute Error (MAE):    {mae:.4f}\")\n",
    "            print(f\"        Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "            print(f\"        R-squared (R²):               {r2:.4f}\")\n",
    "\n",
    "# --- 6. SAVE THE FINAL VALIDATION RESULTS ---\n",
    "print(f\"\\n--- Step 6: Saving final validation results to {FINAL_VALIDATION_OUTPUT_FILE} ---\")\n",
    "validation_df.to_csv(FINAL_VALIDATION_OUTPUT_FILE)\n",
    "print(\"Save complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f213f6-7df7-4de2-a6a8-144acae01cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

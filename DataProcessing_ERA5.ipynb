{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a13f5b-50e8-4b73-b8cb-a4259e8a5b96",
   "metadata": {},
   "source": [
    "### ERA5 Data Processing Script\n",
    "\n",
    "This Python script processes **raw hourly ERA5 GRIB files** into clean, model-ready datasets for a specific location (Minneapolis).\n",
    "\n",
    "##### Key Objectives\n",
    "\n",
    "- **Custom Year Selection**  \n",
    "  - Training: **2000–2020**  \n",
    "  - Testing: **2021–2024**  \n",
    "  - Enforces strict separation to prevent data leakage.\n",
    "\n",
    "- **Location-Specific Extraction**  \n",
    "  - Identifies and extracts data from the **nearest ERA5 grid point** to Minneapolis.\n",
    "\n",
    "- **Variable Harmonization & Unit Conversion**  \n",
    "  - Converts U/V wind components → **scalar wind speed**  \n",
    "  - Air temp & dewpoint → **relative humidity**  \n",
    "  - Cumulative variables (e.g. precip, radiation) → **hourly totals/rates**\n",
    "\n",
    "##### Workflow\n",
    "\n",
    "1. Loop through each year in training & testing periods  \n",
    "2. Read GRIB files, extract hourly series for Minneapolis  \n",
    "3. Apply conversions and standardize variables  \n",
    "4. Concatenate results\n",
    "\n",
    "##### Final Outputs\n",
    "\n",
    "- `ERA5_Train_2000-2020.csv` – Clean hourly data for training  \n",
    "- `ERA5_Test_2021-2024.csv` – Clean hourly data for testing\n",
    "\n",
    "##### Note: \n",
    "- Due to processing reasons, I have to unzip the files manually rather than automating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca26db99-6746-4c4f-957b-72b9d2f2bfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============== PROCESSING TRAINING DATA ===============\n",
      "\n",
      "--- Processing Year: 2000 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2000.\n",
      "\n",
      "--- Processing Year: 2001 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2001.\n",
      "\n",
      "--- Processing Year: 2002 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2002.\n",
      "\n",
      "--- Processing Year: 2003 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2003.\n",
      "\n",
      "--- Processing Year: 2004 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2004.\n",
      "\n",
      "--- Processing Year: 2005 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2005.\n",
      "\n",
      "--- Processing Year: 2006 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2006.\n",
      "\n",
      "--- Processing Year: 2007 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2007.\n",
      "\n",
      "--- Processing Year: 2008 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2008.\n",
      "\n",
      "--- Processing Year: 2009 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2009.\n",
      "\n",
      "--- Processing Year: 2010 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2010.\n",
      "\n",
      "--- Processing Year: 2011 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2011.\n",
      "\n",
      "--- Processing Year: 2012 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2012.\n",
      "\n",
      "--- Processing Year: 2013 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2013.\n",
      "\n",
      "--- Processing Year: 2014 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2014.\n",
      "\n",
      "--- Processing Year: 2015 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2015.\n",
      "\n",
      "--- Processing Year: 2016 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2016.\n",
      "\n",
      "--- Processing Year: 2017 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2017.\n",
      "\n",
      "--- Processing Year: 2018 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2018.\n",
      "\n",
      "--- Processing Year: 2019 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2019.\n",
      "\n",
      "--- Processing Year: 2020 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2020.\n",
      "\n",
      "Combining all processed years into a single DataFrame...\n",
      "\n",
      "--- Final Training DataFrame ---\n",
      "                     air_temperature_k  wind_speed_ms  precip_hourly_mm  \\\n",
      "time                                                                      \n",
      "1999-12-31 18:00:00                NaN            NaN               0.0   \n",
      "2000-01-01 00:00:00         275.169067       5.701304               NaN   \n",
      "2000-01-01 01:00:00         275.382202       5.798653               NaN   \n",
      "2000-01-01 02:00:00         275.477783       5.565651               NaN   \n",
      "2000-01-01 03:00:00         273.959717       5.309109               NaN   \n",
      "\n",
      "                     relative_humidity_percent  solar_radiation_w_m2  \\\n",
      "time                                                                   \n",
      "1999-12-31 18:00:00                        NaN                   0.0   \n",
      "2000-01-01 00:00:00                  65.966998                   NaN   \n",
      "2000-01-01 01:00:00                  69.473722                   NaN   \n",
      "2000-01-01 02:00:00                  72.265052                   NaN   \n",
      "2000-01-01 03:00:00                  77.784653                   NaN   \n",
      "\n",
      "                     thermal_radiation_w_m2  \n",
      "time                                         \n",
      "1999-12-31 18:00:00                0.065237  \n",
      "2000-01-01 00:00:00                     NaN  \n",
      "2000-01-01 01:00:00                     NaN  \n",
      "2000-01-01 02:00:00                     NaN  \n",
      "2000-01-01 03:00:00                     NaN  \n",
      "\n",
      "Training data shape: (184125, 6)\n",
      "\n",
      "Saving training data to: C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\\ERA5_Train_2000-2020.csv\n",
      "Save complete.\n",
      "\n",
      "\n",
      "=============== PROCESSING TESTING DATA ===============\n",
      "\n",
      "--- Processing Year: 2021 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2021.\n",
      "\n",
      "--- Processing Year: 2022 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2022.\n",
      "\n",
      "--- Processing Year: 2023 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2023.\n",
      "\n",
      "--- Processing Year: 2024 ---\n",
      "  Loading raw data from: data.grib\n",
      "  Found nearest grid point for {'lat': 44.98, 'lon': -93.26} at index (4, 3).\n",
      "  Applying unit conversions...\n",
      "  Successfully processed and standardized data for 2024.\n",
      "\n",
      "Combining all processed years into a single DataFrame...\n",
      "\n",
      "--- Final Testing DataFrame ---\n",
      "                     air_temperature_k  wind_speed_ms  precip_hourly_mm  \\\n",
      "time                                                                      \n",
      "2020-12-31 18:00:00                NaN            NaN               0.0   \n",
      "2021-01-01 00:00:00         266.595062       3.790254               NaN   \n",
      "2021-01-01 01:00:00         266.417084       3.746019               NaN   \n",
      "2021-01-01 02:00:00         265.921478       3.002020               NaN   \n",
      "2021-01-01 03:00:00         265.026184       2.203375               NaN   \n",
      "\n",
      "                     relative_humidity_percent  solar_radiation_w_m2  \\\n",
      "time                                                                   \n",
      "2020-12-31 18:00:00                        NaN                   0.0   \n",
      "2021-01-01 00:00:00                  83.286371                   NaN   \n",
      "2021-01-01 01:00:00                  84.254767                   NaN   \n",
      "2021-01-01 02:00:00                  85.764350                   NaN   \n",
      "2021-01-01 03:00:00                  87.262576                   NaN   \n",
      "\n",
      "                     thermal_radiation_w_m2  \n",
      "time                                         \n",
      "2020-12-31 18:00:00                 0.06065  \n",
      "2021-01-01 00:00:00                     NaN  \n",
      "2021-01-01 01:00:00                     NaN  \n",
      "2021-01-01 02:00:00                     NaN  \n",
      "2021-01-01 03:00:00                     NaN  \n",
      "\n",
      "Testing data shape: (35068, 6)\n",
      "\n",
      "Saving testing data to: C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\\ERA5_Test_2021-2024.csv\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pygrib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# This section defines the data sources and the train/test split.\n",
    "\n",
    "ROOT_DATA_DIR = r\"C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\"\n",
    "\n",
    "# Define the years for the training set and the test set\n",
    "TRAIN_YEARS = [2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020]\n",
    "TEST_YEARS = [2021,2022,2023,2024]\n",
    "\n",
    "MINNEAPOLIS_COORDS = {\"lat\": 44.98, \"lon\": -93.26}\n",
    "\n",
    "# This mapping ensures we use consistent names throughout the process.\n",
    "# Using the new, better variable names from your latest download.\n",
    "VARIABLE_MAPPING = {\n",
    "    '2 metre temperature': '2_metre_temperature',\n",
    "    '10 metre U wind component': '10_metre_u_wind_component',\n",
    "    '10 metre V wind component': '10_metre_v_wind_component',\n",
    "    '2 metre dewpoint temperature': '2_metre_dewpoint_temperature',\n",
    "    'Total precipitation': 'total_precipitation',\n",
    "    'Mean surface downward short-wave radiation flux': 'solar_radiation',\n",
    "    'Mean surface downward long-wave radiation flux': 'thermal_radiation'\n",
    "}\n",
    "DESIRED_GRIB_VARIABLES = list(VARIABLE_MAPPING.keys())\n",
    "\n",
    "# --- 2. HELPER & UNIT CONVERSION FUNCTIONS ---\n",
    "\n",
    "def decumulate_hourly(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Robustly de-cumulates an hourly time series that may have forecast resets.\n",
    "    \"\"\"\n",
    "    diffs = series.diff()\n",
    "    # Where diff is negative (a reset), use the original series value.\n",
    "    hourly_values = diffs.where(diffs >= 0, series)\n",
    "    # The first value will be NaN after diff(), so fill it with the original first value.\n",
    "    if len(hourly_values) > 0:\n",
    "        hourly_values.iloc[0] = series.iloc[0]\n",
    "    return hourly_values\n",
    "\n",
    "def standardize_dataframe(raw_df):\n",
    "    \"\"\"\n",
    "    Takes a raw DataFrame from GRIB extraction and converts all variables\n",
    "    to their final, standardized units.\n",
    "    \"\"\"\n",
    "    print(\"  Applying unit conversions...\")\n",
    "    df = raw_df.copy()\n",
    "\n",
    "    df['wind_speed_ms'] = np.sqrt(df[VARIABLE_MAPPING['10 metre U wind component']]**2 + df[VARIABLE_MAPPING['10 metre V wind component']]**2)\n",
    "\n",
    "    precip_m = decumulate_hourly(df[VARIABLE_MAPPING['Total precipitation']])\n",
    "    df['precip_hourly_mm'] = precip_m * 1000\n",
    "\n",
    "    t_air_c = df[VARIABLE_MAPPING['2 metre temperature']] - 273.15\n",
    "    t_dew_c = df[VARIABLE_MAPPING['2 metre dewpoint temperature']] - 273.15\n",
    "    e_s = 0.61094 * np.exp((17.625 * t_air_c) / (t_air_c + 243.04))\n",
    "    e = 0.61094 * np.exp((17.625 * t_dew_c) / (t_dew_c + 243.04))\n",
    "    df['relative_humidity_percent'] = (e / e_s) * 100\n",
    "    df['relative_humidity_percent'] = df['relative_humidity_percent'].clip(0, 100)\n",
    "\n",
    "    solar_rad_j_m2 = decumulate_hourly(df[VARIABLE_MAPPING['Mean surface downward short-wave radiation flux']])\n",
    "    df['solar_radiation_w_m2'] = solar_rad_j_m2 / 3600\n",
    "\n",
    "    thermal_rad_j_m2 = decumulate_hourly(df[VARIABLE_MAPPING['Mean surface downward long-wave radiation flux']])\n",
    "    df['thermal_radiation_w_m2'] = thermal_rad_j_m2 / 3600\n",
    "\n",
    "    df = df.rename(columns={VARIABLE_MAPPING['2 metre temperature']: 'air_temperature_k'})\n",
    "    final_cols = [\n",
    "        'air_temperature_k', 'wind_speed_ms', 'precip_hourly_mm',\n",
    "        'relative_humidity_percent', 'solar_radiation_w_m2', 'thermal_radiation_w_m2'\n",
    "    ]\n",
    "    return df[final_cols]\n",
    "\n",
    "# --- 3. MAIN PROCESSING WORKFLOW ---\n",
    "\n",
    "def extract_raw_data_for_year(grib_file_path, target_coords):\n",
    "    \"\"\"Extracts a raw time series DataFrame for a single point from a GRIB file.\"\"\"\n",
    "    time_series_data = {}\n",
    "    with pygrib.open(grib_file_path) as grbs:\n",
    "        msgs = list(grbs)\n",
    "    if not msgs: return None\n",
    "    \n",
    "    grb_grid_msg = msgs[0]\n",
    "    lats, lons = grb_grid_msg.latlons()\n",
    "    lons_180 = (lons + 180) % 360 - 180\n",
    "    dist_sq = (lats - target_coords[\"lat\"])**2 + (lons_180 - target_coords[\"lon\"])**2\n",
    "    min_dist_idx = np.unravel_index(np.argmin(dist_sq, axis=None), dist_sq.shape)\n",
    "    print(f\"  Found nearest grid point for {target_coords} at index {min_dist_idx}.\")\n",
    "    \n",
    "    for grb in msgs:\n",
    "        if grb.name in DESIRED_GRIB_VARIABLES:\n",
    "            ts = grb.validDate\n",
    "            clean_name = VARIABLE_MAPPING[grb.name]\n",
    "            if ts not in time_series_data: time_series_data[ts] = {'time': ts}\n",
    "            time_series_data[ts][clean_name] = grb.values[min_dist_idx]\n",
    "            \n",
    "    if not time_series_data: return None\n",
    "    df = pd.DataFrame(sorted(time_series_data.values(), key=lambda x: x['time'])).set_index('time')\n",
    "    return df\n",
    "\n",
    "def process_era5_data_for_years(base_dir, years_list):\n",
    "    \"\"\"\n",
    "    Main function to find, extract, and process ERA5 GRIB files for a given list of years.\n",
    "    This version assumes files have already been unzipped.\n",
    "    \"\"\"\n",
    "    all_yearly_data = []\n",
    "    for year in sorted(years_list):\n",
    "        year_str = str(year)\n",
    "        print(f\"\\n--- Processing Year: {year_str} ---\")\n",
    "\n",
    "        # The script now looks for a folder with the year's name directly.\n",
    "        year_folder = os.path.join(base_dir, year_str)\n",
    "        \n",
    "        if not os.path.isdir(year_folder):\n",
    "            print(f\"  Error: Directory not found at '{year_folder}'. Skipping year.\")\n",
    "            continue\n",
    "\n",
    "        # Find the GRIB file within the year's folder\n",
    "        grib_file_path = None\n",
    "        for file in os.listdir(year_folder):\n",
    "            if file.lower().endswith(('.grib', '.grb')):\n",
    "                grib_file_path = os.path.join(year_folder, file)\n",
    "                break\n",
    "        \n",
    "        if not grib_file_path:\n",
    "            print(f\"  Error: No GRIB file found in '{year_folder}'. Skipping year.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Loading raw data from: {os.path.basename(grib_file_path)}\")\n",
    "        raw_df = extract_raw_data_for_year(grib_file_path, MINNEAPOLIS_COORDS)\n",
    "        \n",
    "        if raw_df is None:\n",
    "            print(f\"  Could not extract raw data for {year_str}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        standardized_df = standardize_dataframe(raw_df)\n",
    "        all_yearly_data.append(standardized_df)\n",
    "        print(f\"  Successfully processed and standardized data for {year_str}.\")\n",
    "\n",
    "    if not all_yearly_data:\n",
    "        print(\"\\nNo data was successfully processed.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nCombining all processed years into a single DataFrame...\")\n",
    "    master_df = pd.concat(all_yearly_data)\n",
    "    return master_df\n",
    "\n",
    "# --- 4. EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- Process Training Data ---\n",
    "    print(\"\\n\\n=============== PROCESSING TRAINING DATA ===============\")\n",
    "    train_df = process_era5_data_for_years(ROOT_DATA_DIR, TRAIN_YEARS)\n",
    "    \n",
    "    if train_df is not None:\n",
    "        print(\"\\n--- Final Training DataFrame ---\")\n",
    "        print(train_df.head())\n",
    "        print(f\"\\nTraining data shape: {train_df.shape}\")\n",
    "        \n",
    "        output_path_train = os.path.join(ROOT_DATA_DIR, \"ERA5_Train_2000-2020.csv\")\n",
    "        print(f\"\\nSaving training data to: {output_path_train}\")\n",
    "        train_df.to_csv(output_path_train)\n",
    "        print(\"Save complete.\")\n",
    "\n",
    "    # --- Process Testing Data ---\n",
    "    print(\"\\n\\n=============== PROCESSING TESTING DATA ===============\")\n",
    "    test_df = process_era5_data_for_years(ROOT_DATA_DIR, TEST_YEARS)\n",
    "\n",
    "    if test_df is not None:\n",
    "        print(\"\\n--- Final Testing DataFrame ---\")\n",
    "        print(test_df.head())\n",
    "        print(f\"\\nTesting data shape: {test_df.shape}\")\n",
    "\n",
    "        output_path_test = os.path.join(ROOT_DATA_DIR, \"ERA5_Test_2021-2024.csv\")\n",
    "        print(f\"\\nSaving testing data to: {output_path_test}\")\n",
    "        test_df.to_csv(output_path_test)\n",
    "        print(\"Save complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059966e9-8c32-4ea2-bdfb-eba740301555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============== PREPARING TRAINING DATA ===============\n",
      "  Creating daily summary predictors (X)...\n",
      "  Creating pivoted hourly targets (y)...\n",
      "  Joining predictors and targets...\n",
      "\n",
      "--- Final Training Data for Modeling ---\n",
      "            air_temperature_k_mean  air_temperature_k_min  \\\n",
      "2000-01-01              271.268669             268.519287   \n",
      "2000-01-02              271.413062             269.948181   \n",
      "2000-01-03              269.083822             267.373535   \n",
      "2000-01-04              265.343854             261.433716   \n",
      "2000-01-05              263.126009             258.798325   \n",
      "\n",
      "            air_temperature_k_max  wind_speed_ms_mean  wind_speed_ms_max  \\\n",
      "2000-01-01             275.477783            3.805613           5.798653   \n",
      "2000-01-02             274.148315            4.017749           5.433529   \n",
      "2000-01-03             271.614990            2.997503           3.560448   \n",
      "2000-01-04             269.044189            4.446761           5.871527   \n",
      "2000-01-05             268.758179            3.885154           6.121527   \n",
      "\n",
      "            wind_speed_ms_std  relative_humidity_percent_mean  \\\n",
      "2000-01-01           0.962650                       75.345473   \n",
      "2000-01-02           0.633816                       84.417143   \n",
      "2000-01-03           0.382638                       79.936153   \n",
      "2000-01-04           0.916097                       73.876465   \n",
      "2000-01-05           1.537608                       72.297519   \n",
      "\n",
      "            solar_radiation_w_m2_mean  thermal_radiation_w_m2_mean  \\\n",
      "2000-01-01                   0.043572                     0.069263   \n",
      "2000-01-02                   0.026480                     0.076455   \n",
      "2000-01-03                   0.017370                     0.076986   \n",
      "2000-01-04                   0.045260                     0.053705   \n",
      "2000-01-05                   0.015109                     0.077831   \n",
      "\n",
      "            precip_hourly_mm_sum  ...  wind_speed_ms_14  wind_speed_ms_15  \\\n",
      "2000-01-01              0.002861  ...          2.870781          2.982524   \n",
      "2000-01-02              0.038147  ...          4.185392          4.486055   \n",
      "2000-01-03              0.064850  ...          3.023194          3.132343   \n",
      "2000-01-04              0.000000  ...          4.613102          4.944033   \n",
      "2000-01-05              0.287533  ...          4.898634          4.917340   \n",
      "\n",
      "            wind_speed_ms_16  wind_speed_ms_17  wind_speed_ms_18  \\\n",
      "2000-01-01          3.486052          3.347930          3.228330   \n",
      "2000-01-02          4.760189          5.433529          5.050147   \n",
      "2000-01-03          3.201383          2.611097          2.434730   \n",
      "2000-01-04          5.011500          4.607321          4.165257   \n",
      "2000-01-05          5.029992          5.553275          5.885766   \n",
      "\n",
      "            wind_speed_ms_19  wind_speed_ms_20  wind_speed_ms_21  \\\n",
      "2000-01-01          3.291499          3.463370          3.626440   \n",
      "2000-01-02          4.770605          4.774601          4.316103   \n",
      "2000-01-03          2.936667          3.185240          3.240957   \n",
      "2000-01-04          4.256040          4.310868          3.591727   \n",
      "2000-01-05          6.121527          5.697458          5.293166   \n",
      "\n",
      "            wind_speed_ms_22  wind_speed_ms_23  \n",
      "2000-01-01          3.104856          3.676749  \n",
      "2000-01-02          3.163390          3.094808  \n",
      "2000-01-03          3.248834          3.560448  \n",
      "2000-01-04          3.184939          2.002296  \n",
      "2000-01-05          4.702824          4.988392  \n",
      "\n",
      "[5 rows x 88 columns]\n",
      "\n",
      "Training modeling data shape: (7671, 88)\n",
      "\n",
      "Saving model-ready training data to: C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\\MODELING_Train_2000-2020.csv\n",
      "Save complete.\n",
      "\n",
      "\n",
      "=============== PREPARING TESTING DATA ===============\n",
      "  Creating daily summary predictors (X)...\n",
      "  Creating pivoted hourly targets (y)...\n",
      "  Joining predictors and targets...\n",
      "\n",
      "--- Final Testing Data for Modeling ---\n",
      "            air_temperature_k_mean  air_temperature_k_min  \\\n",
      "2021-01-01              264.796181             263.204529   \n",
      "2021-01-02              266.005541             263.044312   \n",
      "2021-01-03              267.505527             264.162323   \n",
      "2021-01-04              270.559372             268.215210   \n",
      "2021-01-05              268.633504             262.788208   \n",
      "\n",
      "            air_temperature_k_max  wind_speed_ms_mean  wind_speed_ms_max  \\\n",
      "2021-01-01             266.754639            2.243019           3.964349   \n",
      "2021-01-02             270.092896            2.150823           3.196464   \n",
      "2021-01-03             271.429565            3.436489           3.940796   \n",
      "2021-01-04             276.783569            3.547267           4.393787   \n",
      "2021-01-05             273.473480            2.249974           3.849004   \n",
      "\n",
      "            wind_speed_ms_std  relative_humidity_percent_mean  \\\n",
      "2021-01-01           1.203473                       86.388295   \n",
      "2021-01-02           0.438294                       87.743572   \n",
      "2021-01-03           0.293924                       89.831449   \n",
      "2021-01-04           0.458433                       86.910874   \n",
      "2021-01-05           1.066919                       86.821140   \n",
      "\n",
      "            solar_radiation_w_m2_mean  thermal_radiation_w_m2_mean  \\\n",
      "2021-01-01                   0.034731                     0.067631   \n",
      "2021-01-02                   0.043759                     0.073846   \n",
      "2021-01-03                   0.030781                     0.079868   \n",
      "2021-01-04                   0.040143                     0.067013   \n",
      "2021-01-05                   0.049874                     0.064526   \n",
      "\n",
      "            precip_hourly_mm_sum  ...  wind_speed_ms_14  wind_speed_ms_15  \\\n",
      "2021-01-01              0.000477  ...          1.663108          1.944490   \n",
      "2021-01-02              0.001907  ...          2.086550          2.093906   \n",
      "2021-01-03              0.001431  ...          3.656894          3.365894   \n",
      "2021-01-04              0.000000  ...          3.069471          3.188754   \n",
      "2021-01-05              0.000000  ...          1.984120          1.865141   \n",
      "\n",
      "            wind_speed_ms_16  wind_speed_ms_17  wind_speed_ms_18  \\\n",
      "2021-01-01          0.922590          1.007132          0.418981   \n",
      "2021-01-02          1.670267          1.648530          1.895339   \n",
      "2021-01-03          3.140955          3.131463          3.453497   \n",
      "2021-01-04          3.018020          2.848819          2.895230   \n",
      "2021-01-05          0.509588          0.354235          0.550712   \n",
      "\n",
      "            wind_speed_ms_19  wind_speed_ms_20  wind_speed_ms_21  \\\n",
      "2021-01-01          1.570987          0.675745          0.229917   \n",
      "2021-01-02          2.086474          2.530201          2.993395   \n",
      "2021-01-03          3.663275          3.911558          3.629354   \n",
      "2021-01-04          3.019246          2.874024          3.723655   \n",
      "2021-01-05          0.855018          1.192961          1.229956   \n",
      "\n",
      "            wind_speed_ms_22  wind_speed_ms_23  \n",
      "2021-01-01          1.000670          1.342387  \n",
      "2021-01-02          3.060137          3.196464  \n",
      "2021-01-03          3.081998          3.324519  \n",
      "2021-01-04          3.384974          3.751591  \n",
      "2021-01-05          1.945994          1.990495  \n",
      "\n",
      "[5 rows x 88 columns]\n",
      "\n",
      "Testing modeling data shape: (1461, 88)\n",
      "\n",
      "Saving model-ready testing data to: C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\\MODELING_Test_2021-2024.csv\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Define the location of your processed data files.\n",
    "ROOT_DATA_DIR = r\"C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\"\n",
    "TRAIN_INPUT_FILE = os.path.join(ROOT_DATA_DIR, \"ERA5_Train_2000-2020.csv\")\n",
    "TEST_INPUT_FILE = os.path.join(ROOT_DATA_DIR, \"ERA5_Test_2021-2024.csv\")\n",
    "\n",
    "# Define where to save the final, model-ready data\n",
    "TRAIN_OUTPUT_FILE = os.path.join(ROOT_DATA_DIR, \"MODELING_Train_2000-2020.csv\")\n",
    "TEST_OUTPUT_FILE = os.path.join(ROOT_DATA_DIR, \"MODELING_Test_2021-2024.csv\")\n",
    "\n",
    "\n",
    "# --- 2. DATA STRUCTURING FUNCTION ---\n",
    "\n",
    "def create_modeling_datasets(hourly_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Takes a clean hourly time series and structures it into two key datasets:\n",
    "    1. A daily summary DataFrame to be used as model predictors (X).\n",
    "    2. A pivoted hourly DataFrame to be used as model targets (y).\n",
    "    \"\"\"\n",
    "    print(\"  Creating daily summary predictors (X)...\")\n",
    "    # --- Create Daily Predictors (X) ---\n",
    "    # Resample the hourly data to a daily frequency ('D').\n",
    "    # For each variable, calculate the relevant summary statistics.\n",
    "    daily_predictors = hourly_df.resample('D').agg(\n",
    "        # For temperature, we want the mean, min, and max for the day.\n",
    "        air_temperature_k_mean=('air_temperature_k', 'mean'),\n",
    "        air_temperature_k_min=('air_temperature_k', 'min'),\n",
    "        air_temperature_k_max=('air_temperature_k', 'max'),\n",
    "        # For wind, we want mean, max, and standard deviation (gustiness).\n",
    "        wind_speed_ms_mean=('wind_speed_ms', 'mean'),\n",
    "        wind_speed_ms_max=('wind_speed_ms', 'max'),\n",
    "        wind_speed_ms_std=('wind_speed_ms', 'std'),\n",
    "        # For other variables, the daily mean is sufficient.\n",
    "        relative_humidity_percent_mean=('relative_humidity_percent', 'mean'),\n",
    "        solar_radiation_w_m2_mean=('solar_radiation_w_m2', 'mean'),\n",
    "        thermal_radiation_w_m2_mean=('thermal_radiation_w_m2', 'mean'),\n",
    "        # For precipitation, we want the total sum for the day.\n",
    "        precip_hourly_mm_sum=('precip_hourly_mm', 'sum')\n",
    "    ).dropna() # Drop any days that might have missing data\n",
    "\n",
    "\n",
    "    print(\"  Creating pivoted hourly targets (y)...\")\n",
    "    # --- Create Hourly Targets (y) ---\n",
    "    # To create our target columns (e.g., temp_hour_0, temp_hour_1...),\n",
    "    # we first need to add 'date' and 'hour' columns to the original data.\n",
    "    df_for_pivot = hourly_df.copy()\n",
    "    df_for_pivot['date'] = df_for_pivot.index.date\n",
    "    df_for_pivot['hour'] = df_for_pivot.index.hour\n",
    "    \n",
    "    # Pivot the table to create one row per day and 24 columns for each hour\n",
    "    hourly_targets = pd.pivot_table(\n",
    "        df_for_pivot,\n",
    "        index='date',\n",
    "        columns='hour',\n",
    "        # List all the variables you want to have hourly columns for\n",
    "        values=['air_temperature_k', 'wind_speed_ms', 'precip_hourly_mm',\n",
    "                'relative_humidity_percent', 'solar_radiation_w_m2', 'thermal_radiation_w_m2']\n",
    "    )\n",
    "    # The pivot creates multi-level columns; flatten them into a single level.\n",
    "    hourly_targets.columns = [f\"{var}_{hour}\" for var, hour in hourly_targets.columns]\n",
    "    \n",
    "    \n",
    "    print(\"  Joining predictors and targets...\")\n",
    "    # --- Combine into Final Modeling DataFrame ---\n",
    "    # Merge the daily predictors and hourly targets on the date index.\n",
    "    # This ensures perfect alignment between our X and y data.\n",
    "    modeling_df = daily_predictors.join(hourly_targets, how='inner')\n",
    "    \n",
    "    return modeling_df\n",
    "\n",
    "# --- 3. EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- Process Training Data ---\n",
    "    print(\"\\n=============== PREPARING TRAINING DATA ===============\")\n",
    "    try:\n",
    "        train_hourly_df = pd.read_csv(TRAIN_INPUT_FILE, index_col='time', parse_dates=True)\n",
    "        train_modeling_df = create_modeling_datasets(train_hourly_df)\n",
    "        \n",
    "        print(\"\\n--- Final Training Data for Modeling ---\")\n",
    "        print(train_modeling_df.head())\n",
    "        print(f\"\\nTraining modeling data shape: {train_modeling_df.shape}\")\n",
    "        \n",
    "        print(f\"\\nSaving model-ready training data to: {TRAIN_OUTPUT_FILE}\")\n",
    "        train_modeling_df.to_csv(TRAIN_OUTPUT_FILE)\n",
    "        print(\"Save complete.\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Training file not found at {TRAIN_INPUT_FILE}\")\n",
    "\n",
    "    # --- Process Testing Data ---\n",
    "    print(\"\\n\\n=============== PREPARING TESTING DATA ===============\")\n",
    "    try:\n",
    "        test_hourly_df = pd.read_csv(TEST_INPUT_FILE, index_col='time', parse_dates=True)\n",
    "        test_modeling_df = create_modeling_datasets(test_hourly_df)\n",
    "        \n",
    "        print(\"\\n--- Final Testing Data for Modeling ---\")\n",
    "        print(test_modeling_df.head())\n",
    "        print(f\"\\nTesting modeling data shape: {test_modeling_df.shape}\")\n",
    "\n",
    "        print(f\"\\nSaving model-ready testing data to: {TEST_OUTPUT_FILE}\")\n",
    "        test_modeling_df.to_csv(TEST_OUTPUT_FILE)\n",
    "        print(\"Save complete.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Testing file not found at {TEST_INPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e04852c3-8b83-471e-8787-ca5301c32cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# import joblib # Used for saving and loading models\n",
    "\n",
    "# # --- 1. CONFIGURATION ---\n",
    "# # Define the location of your modeling-ready data files.\n",
    "# ROOT_DATA_DIR = r\"C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\"\n",
    "# TRAIN_MODELING_FILE = os.path.join(ROOT_DATA_DIR, \"MODELING_Train_2000-2014.csv\")\n",
    "# TEST_MODELING_FILE = os.path.join(ROOT_DATA_DIR, \"MODELING_Test_2015-18.csv\")\n",
    "# MODEL_SAVE_DIR = os.path.join(ROOT_DATA_DIR, \"trained_models\")\n",
    "# PREDICTIONS_OUTPUT_FILE = os.path.join(ROOT_DATA_DIR, \"VALIDATION_Predictions_2015-18.csv\")\n",
    "\n",
    "# # Create a directory to save our trained models\n",
    "# os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# # --- 2. LOAD PREPARED DATA ---\n",
    "# print(\"--- Step 1: Loading prepared modeling data ---\")\n",
    "# try:\n",
    "#     train_df = pd.read_csv(TRAIN_MODELING_FILE, index_col=0, parse_dates=True)\n",
    "#     test_df = pd.read_csv(TEST_MODELING_FILE, index_col=0, parse_dates=True)\n",
    "#     print(\"  Successfully loaded training and testing data.\")\n",
    "# except FileNotFoundError as e:\n",
    "#     print(f\"Error: Could not find data file at {e.filename}.\")\n",
    "#     print(\"Please ensure you have run the previous data preparation script successfully.\")\n",
    "#     exit() # Exit if data isn't found\n",
    "\n",
    "\n",
    "# # --- 3. TRAIN THE MODELS ---\n",
    "# print(\"\\n--- Step 2: Training all downscaling models ---\")\n",
    "\n",
    "# # A dictionary to hold all our trained models\n",
    "# trained_models = {}\n",
    "\n",
    "# # --- 3a. Train \"Daily Characteristics\" Models for Wind (Two-Stage Approach) ---\n",
    "# print(\"  Training Stage-1 wind characteristic models...\")\n",
    "# if 'wind_speed_ms_mean' in train_df.columns and 'wind_speed_ms_max' in train_df.columns and 'wind_speed_ms_std' in train_df.columns:\n",
    "#     wind_predictors_train = train_df[['wind_speed_ms_mean']]\n",
    "#     wind_target_max_train = train_df['wind_speed_ms_max']\n",
    "#     wind_target_std_train = train_df['wind_speed_ms_std']\n",
    "\n",
    "#     model_wind_max = LinearRegression()\n",
    "#     model_wind_max.fit(wind_predictors_train, wind_target_max_train)\n",
    "#     trained_models['wind_max_from_mean'] = model_wind_max\n",
    "#     joblib.dump(model_wind_max, os.path.join(MODEL_SAVE_DIR, 'model_wind_max.pkl'))\n",
    "\n",
    "#     model_wind_std = LinearRegression()\n",
    "#     model_wind_std.fit(wind_predictors_train, wind_target_std_train)\n",
    "#     trained_models['wind_std_from_mean'] = model_wind_std\n",
    "#     joblib.dump(model_wind_std, os.path.join(MODEL_SAVE_DIR, 'model_wind_std.pkl'))\n",
    "#     print(\"  Wind characteristic models trained and saved.\")\n",
    "# else:\n",
    "#     print(\"  Warning: Could not train wind characteristic models due to missing columns.\")\n",
    "\n",
    "\n",
    "# # --- 3b. Train Main Hourly Models for Each Variable ---\n",
    "# predictor_map = {\n",
    "#     'air_temperature_k': ['air_temperature_k_mean', 'air_temperature_k_min', 'air_temperature_k_max'],\n",
    "#     'wind_speed_ms': ['wind_speed_ms_mean', 'wind_speed_ms_max', 'wind_speed_ms_std'],\n",
    "#     'relative_humidity_percent': ['relative_humidity_percent_mean'],\n",
    "#     'solar_radiation_w_m2': ['solar_radiation_w_m2_mean'],\n",
    "#     'thermal_radiation_w_m2': ['thermal_radiation_w_m2_mean'],\n",
    "#     'precip_hourly_mm': ['precip_hourly_mm_sum']\n",
    "# }\n",
    "\n",
    "# for var_name, predictors in predictor_map.items():\n",
    "#     print(f\"  Training 24 hourly models for: {var_name}...\")\n",
    "#     trained_models[var_name] = {}\n",
    "#     if not all(p in train_df.columns for p in predictors):\n",
    "#         print(f\"    Warning: Not all predictor columns for '{var_name}' found. Skipping.\")\n",
    "#         continue\n",
    "#     for hour in range(24):\n",
    "#         target_col = f\"{var_name}_{hour}\"\n",
    "#         if target_col not in train_df.columns:\n",
    "#             continue\n",
    "#         X_train = train_df[predictors]\n",
    "#         y_train = train_df[target_col]\n",
    "#         model = LinearRegression()\n",
    "#         model.fit(X_train, y_train)\n",
    "#         trained_models[var_name][hour] = model\n",
    "#     joblib.dump(trained_models[var_name], os.path.join(MODEL_SAVE_DIR, f'models_{var_name}.pkl'))\n",
    "\n",
    "# print(\"--- All models have been trained successfully. ---\")\n",
    "\n",
    "\n",
    "# # --- 3.5. NEW: EVALUATE MODEL FIT ON TRAINING DATA (2005-2007) ---\n",
    "# print(\"\\n--- Step 2.5: Evaluating model fit on the training data ---\")\n",
    "\n",
    "# train_predictions = {}\n",
    "# for var_name, predictors in predictor_map.items():\n",
    "#     if var_name not in trained_models or not trained_models[var_name]: continue\n",
    "    \n",
    "#     hourly_preds_list = []\n",
    "#     X_train_fit = train_df[predictors]\n",
    "#     for hour in range(24):\n",
    "#         model = trained_models[var_name].get(hour)\n",
    "#         if model:\n",
    "#             preds = model.predict(X_train_fit)\n",
    "#             hourly_preds_list.append(pd.Series(preds, index=X_train_fit.index, name=hour))\n",
    "    \n",
    "#     if hourly_preds_list:\n",
    "#         var_df_wide = pd.concat(hourly_preds_list, axis=1)\n",
    "#         var_stacked = var_df_wide.stack()\n",
    "#         var_stacked.index = var_stacked.index.map(lambda x: x[0] + pd.to_timedelta(x[1], unit='h'))\n",
    "#         train_predictions[f'predicted_{var_name}'] = var_stacked\n",
    "\n",
    "# train_predictions_df = pd.DataFrame(train_predictions)\n",
    "\n",
    "# # Prepare actuals from training data for comparison\n",
    "# train_actuals_df = pd.DataFrame()\n",
    "# for var_name in predictor_map.keys():\n",
    "#     actual_cols = [f\"{var_name}_{h}\" for h in range(24) if f\"{var_name}_{h}\" in train_df.columns]\n",
    "#     if not actual_cols: continue\n",
    "#     actual_hourly = train_df[actual_cols]\n",
    "#     actual_hourly.columns = [int(c.split('_')[-1]) for c in actual_cols]\n",
    "#     actual_stacked = actual_hourly.stack()\n",
    "#     actual_stacked.index = actual_stacked.index.map(lambda x: x[0] + pd.to_timedelta(x[1], unit='h'))\n",
    "#     train_actuals_df[f'actual_{var_name}'] = actual_stacked\n",
    "\n",
    "# train_validation_df = pd.merge(train_actuals_df, train_predictions_df, left_index=True, right_index=True, how=\"inner\")\n",
    "\n",
    "# print(\"  Training Set Validation Results (Goodness of Fit):\")\n",
    "# for var_name in predictor_map.keys():\n",
    "#     actual_col = f'actual_{var_name}'\n",
    "#     predicted_col = f'predicted_{var_name}'\n",
    "#     if actual_col in train_validation_df.columns and predicted_col in train_validation_df.columns:\n",
    "#         temp_compare_df = train_validation_df[[actual_col, predicted_col]].dropna()\n",
    "#         if not temp_compare_df.empty:\n",
    "#             mae = mean_absolute_error(temp_compare_df[actual_col], temp_compare_df[predicted_col])\n",
    "#             rmse = np.sqrt(mean_squared_error(temp_compare_df[actual_col], temp_compare_df[predicted_col]))\n",
    "#             print(f\"    - {var_name}:\")\n",
    "#             print(f\"        MAE:  {mae:.4f}\")\n",
    "#             print(f\"        RMSE: {rmse:.4f}\")\n",
    "\n",
    "# # --- 4. MAKE PREDICTIONS ON THE TEST SET (2008) ---\n",
    "# print(\"\\n--- Step 3: Generating hourly predictions for the test year (2008) ---\")\n",
    "\n",
    "# X_test_base = test_df.copy()\n",
    "\n",
    "# if 'wind_max_from_mean' in trained_models and 'wind_std_from_mean' in trained_models:\n",
    "#     print(\"  Generating wind characteristic predictions for test set...\")\n",
    "#     X_test_wind_mean = test_df[['wind_speed_ms_mean']]\n",
    "#     predicted_wind_max = trained_models['wind_max_from_mean'].predict(X_test_wind_mean)\n",
    "#     predicted_wind_std = trained_models['wind_std_from_mean'].predict(X_test_wind_mean)\n",
    "#     X_test_base['wind_speed_ms_max'] = predicted_wind_max\n",
    "#     X_test_base['wind_speed_ms_std'] = predicted_wind_std\n",
    "#     print(\"  Wind characteristics generated.\")\n",
    "\n",
    "# final_predictions = {}\n",
    "# for var_name, predictors in predictor_map.items():\n",
    "#     if var_name not in trained_models or not trained_models[var_name]:\n",
    "#         print(f\"  Skipping prediction for '{var_name}' as no models were trained.\")\n",
    "#         continue\n",
    "#     if not all(p in X_test_base.columns for p in predictors):\n",
    "#         print(f\"    Warning: Not all predictor columns for '{var_name}' found in test set. Skipping prediction.\")\n",
    "#         continue\n",
    "#     print(f\"  Predicting hourly values for: {var_name}...\")\n",
    "#     hourly_preds_list = []\n",
    "#     for hour in range(24):\n",
    "#         model = trained_models[var_name].get(hour)\n",
    "#         if model:\n",
    "#             X_test_var = X_test_base[predictors]\n",
    "#             preds = model.predict(X_test_var)\n",
    "#             hourly_preds_list.append(pd.Series(preds, index=X_test_base.index, name=hour))\n",
    "#     if hourly_preds_list:\n",
    "#         var_df_wide = pd.concat(hourly_preds_list, axis=1)\n",
    "#         var_stacked = var_df_wide.stack()\n",
    "#         var_stacked.index = var_stacked.index.map(lambda x: x[0] + pd.to_timedelta(x[1], unit='h'))\n",
    "#         final_predictions[f'predicted_{var_name}'] = var_stacked\n",
    "# predictions_df = pd.DataFrame(final_predictions)\n",
    "# print(\"--- Hourly predictions generated successfully. ---\")\n",
    "\n",
    "\n",
    "# # --- 5. VALIDATE PREDICTIONS ON TEST DATA ---\n",
    "# if not predictions_df.empty:\n",
    "#     print(\"\\n--- Step 4: Validating predictions against actual 2008 data (Test Set) ---\")\n",
    "    \n",
    "#     actuals_df = pd.DataFrame()\n",
    "#     for var_name in predictor_map.keys():\n",
    "#         actual_cols = [f\"{var_name}_{h}\" for h in range(24) if f\"{var_name}_{h}\" in test_df.columns]\n",
    "#         if not actual_cols: continue\n",
    "#         actual_hourly = test_df[actual_cols]\n",
    "#         actual_hourly.columns = [int(c.split('_')[-1]) for c in actual_cols]\n",
    "#         actual_stacked = actual_hourly.stack()\n",
    "#         actual_stacked.index = actual_stacked.index.map(lambda x: x[0] + pd.to_timedelta(x[1], unit='h'))\n",
    "#         actuals_df[f'actual_{var_name}'] = actual_stacked\n",
    "        \n",
    "#     validation_df = pd.merge(actuals_df, predictions_df, left_index=True, right_index=True, how=\"inner\")\n",
    "    \n",
    "#     print(\"  Test Set Validation Results (Generalization Performance):\")\n",
    "#     for var_name in predictor_map.keys():\n",
    "#         actual_col = f'actual_{var_name}'\n",
    "#         predicted_col = f'predicted_{var_name}'\n",
    "#         if actual_col in validation_df.columns and predicted_col in validation_df.columns:\n",
    "#             temp_compare_df = validation_df[[actual_col, predicted_col]].dropna()\n",
    "#             if not temp_compare_df.empty:\n",
    "#                 mae = mean_absolute_error(temp_compare_df[actual_col], temp_compare_df[predicted_col])\n",
    "#                 rmse = np.sqrt(mean_squared_error(temp_compare_df[actual_col], temp_compare_df[predicted_col]))\n",
    "#                 print(f\"    - {var_name}:\")\n",
    "#                 print(f\"        MAE:  {mae:.4f}\")\n",
    "#                 print(f\"        RMSE: {rmse:.4f}\")\n",
    "#             else:\n",
    "#                 print(f\"    - {var_name}: Could not be calculated.\")\n",
    "\n",
    "#     # --- 6. SAVE FINAL PREDICTIONS ---\n",
    "#     print(f\"\\n--- Step 5: Saving final 2008 predictions to {PREDICTIONS_OUTPUT_FILE} ---\")\n",
    "#     validation_df.to_csv(PREDICTIONS_OUTPUT_FILE)\n",
    "#     print(\"Save complete.\")\n",
    "# else:\n",
    "#     print(\"\\nSkipping validation and saving because no predictions were generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a78f0c7-8271-4d99-95d6-017b710309ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "\n",
    "# # --- 1. CONFIGURATION ---\n",
    "# # Define the location of your validation file.\n",
    "# ROOT_DATA_DIR = r\"C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\"\n",
    "# VALIDATION_FILE = os.path.join(ROOT_DATA_DIR, \"VALIDATION_Predictions_2015-18.csv\")\n",
    "# PLOT_SAVE_DIR = os.path.join(ROOT_DATA_DIR, \"validation_plots_Jan2016_1-3\")\n",
    "\n",
    "# # Create a directory to save the plots\n",
    "# os.makedirs(PLOT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# # --- 2. DATA LOADING ---\n",
    "# print(\"--- Loading validation data ---\")\n",
    "# try:\n",
    "#     # FIXED: Use index_col=0 to specify that the first column is the index,\n",
    "#     # regardless of its header name.\n",
    "#     validation_df = pd.read_csv(VALIDATION_FILE, index_col=0, parse_dates=True)\n",
    "#     print(\"  Successfully loaded validation data.\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Error: Validation file not found at {VALIDATION_FILE}\")\n",
    "#     exit()\n",
    "\n",
    "# # --- 3. PLOTTING FUNCTION ---\n",
    "\n",
    "# def plot_validation_timeseries(df, var_name, start_date, end_date):\n",
    "#     \"\"\"\n",
    "#     Creates and saves a time series plot comparing actual vs. predicted values\n",
    "#     for a specific variable and date range.\n",
    "#     \"\"\"\n",
    "#     actual_col = f'actual_{var_name}'\n",
    "#     predicted_col = f'predicted_{var_name}'\n",
    "    \n",
    "#     # Check if both required columns exist in the DataFrame\n",
    "#     if not all(col in df.columns for col in [actual_col, predicted_col]):\n",
    "#         print(f\"\\nSkipping plot for '{var_name}': one or both columns not found.\")\n",
    "#         return\n",
    "\n",
    "#     # Filter the DataFrame for the desired date range\n",
    "#     plot_df = df.loc[start_date:end_date]\n",
    "    \n",
    "#     if plot_df.empty:\n",
    "#         print(f\"\\nNo data found for '{var_name}' in the date range {start_date} to {end_date}.\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"\\nGenerating plot for '{var_name}' from {start_date} to {end_date}...\")\n",
    "\n",
    "#     # Create the plot\n",
    "#     plt.style.use('seaborn-v0_8-whitegrid')\n",
    "#     fig, ax = plt.subplots(figsize=(15, 7))\n",
    "    \n",
    "#     # Plot the actual and predicted lines\n",
    "#     ax.plot(plot_df.index, plot_df[actual_col], label='Actual (ERA5)', color='blue', linewidth=2, marker='o', markersize=4)\n",
    "#     ax.plot(plot_df.index, plot_df[predicted_col], label='Predicted (Model)', color='red', linewidth=1.5, linestyle='--')\n",
    "    \n",
    "#     # Formatting the plot\n",
    "#     plt.title(f'Actual vs. Predicted Hourly {var_name.replace(\"_\", \" \").title()}\\n({start_date} to {end_date})', fontsize=16)\n",
    "#     plt.ylabel(var_name.split('_')[-1].upper(), fontsize=12)\n",
    "#     plt.xlabel('Date and Time', fontsize=12)\n",
    "#     plt.legend(fontsize=12)\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     # Save the plot to a file\n",
    "#     plot_filename = f\"Validation_Plot_{var_name}_{start_date}_to_{end_date}.png\"\n",
    "#     save_path = os.path.join(PLOT_SAVE_DIR, plot_filename)\n",
    "#     plt.savefig(save_path)\n",
    "#     print(f\"  Plot saved to: {save_path}\")\n",
    "#     plt.close(fig) # Close the figure to free up memory\n",
    "\n",
    "\n",
    "# # --- 4. EXECUTION ---\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "#     # Define the date range for the plots (e.g., the first 3 days of the year)\n",
    "#     start_plot_date = '2016-01-01'\n",
    "#     end_plot_date = '2016-01-03'\n",
    "\n",
    "#     # Generate plots for all key variables\n",
    "#     variables_to_plot = [\n",
    "#         'air_temperature_k',\n",
    "#         'wind_speed_ms',\n",
    "#         'relative_humidity_percent',\n",
    "#         'precip_hourly_mm',\n",
    "#         'solar_radiation_w_m2',\n",
    "#         'thermal_radiation_w_m2'\n",
    "#     ]\n",
    "\n",
    "#     for variable in variables_to_plot:\n",
    "#         plot_validation_timeseries(validation_df, variable, start_plot_date, end_plot_date)\n",
    "        \n",
    "#     print(\"\\n--- All plots generated successfully. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac074dd-4143-4a8e-980b-652077554da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b1a30a5-7e4d-4368-8332-fd702dd818cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea2bb5e-d9da-4e8e-ab84-1331b2b9b441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eeb733-f870-4fa5-98c6-87bdea6b38cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e84806b-e84b-4384-a4d7-2e08d5e74cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# import joblib\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # --- 1. CONFIGURATION ---\n",
    "# # Define the paths for your input files and trained models.\n",
    "# ROOT_DATA_DIR = r\"C:\\Users\\91788\\Downloads\\ERA5 Data\\Extracted\" \n",
    "# MODEL_SAVE_DIR = os.path.join(ROOT_DATA_DIR, \"trained_models\")\n",
    "\n",
    "# # Input files for this final validation run\n",
    "# NASA_DAILY_INPUT_FILE = os.path.join(ROOT_DATA_DIR, \"NASA_Standardized_Minnesota_2015-2018.csv\")\n",
    "# ERA5_HOURLY_GROUND_TRUTH_FILE = os.path.join(ROOT_DATA_DIR, \"ERA5_Standardized_Minneapolis_2015-2018.csv\") \n",
    "\n",
    "# # Target coordinates for filtering the NASA data\n",
    "# TARGET_LAT = 44.98\n",
    "# TARGET_LON = -93.26\n",
    "\n",
    "# # Output files\n",
    "# FINAL_VALIDATION_OUTPUT_FILE = os.path.join(ROOT_DATA_DIR, \"FINAL_VALIDATION_NASA_vs_ERA5_2015-2018.csv\")\n",
    "# PLOT_SAVE_DIR = os.path.join(ROOT_DATA_DIR, \"final_validation_plots\")\n",
    "# os.makedirs(PLOT_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# # --- 2. LOAD DATA AND TRAINED MODELS ---\n",
    "# print(\"--- Step 1: Loading data and trained models for FINAL validation ---\")\n",
    "\n",
    "# # Load the multi-point, daily NASA data\n",
    "# try:\n",
    "#     nasa_df_full = pd.read_csv(NASA_DAILY_INPUT_FILE)\n",
    "#     nasa_df_full['time'] = pd.to_datetime(nasa_df_full['time'])\n",
    "#     print(\"  Successfully loaded full NASA daily data (2015-2018).\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Error: NASA data file not found at {NASA_DAILY_INPUT_FILE}.\")\n",
    "#     exit()\n",
    "\n",
    "# # Load the single-point, hourly ERA5 data (our ground truth)\n",
    "# try:\n",
    "#     era5_df = pd.read_csv(ERA5_HOURLY_GROUND_TRUTH_FILE, index_col='time', parse_dates=True)\n",
    "#     print(\"  Successfully loaded ERA5 hourly ground truth data (2015-2018).\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Error: ERA5 ground truth file not found at {ERA5_HOURLY_GROUND_TRUTH_FILE}.\")\n",
    "#     exit()\n",
    "\n",
    "# # Load the library of trained models\n",
    "# try:\n",
    "#     print(\"  Loading pre-trained models...\")\n",
    "#     predictor_map = {\n",
    "#         'air_temperature_k': ['tas', 'tasmin', 'tasmax'],\n",
    "#         'wind_speed_ms': ['sfcWind_mean', 'sfcWind_max', 'sfcWind_std'],\n",
    "#         'relative_humidity_percent': ['hurs'],\n",
    "#         'solar_radiation_w_m2': ['rsds'],\n",
    "#         'thermal_radiation_w_m2': ['rlds'],\n",
    "#         'precip_hourly_mm': ['precip_daily_mm']\n",
    "#     }\n",
    "#     trained_models = {}\n",
    "#     for var_name in predictor_map.keys():\n",
    "#         model_path = os.path.join(MODEL_SAVE_DIR, f'models_{var_name}.pkl')\n",
    "#         trained_models[var_name] = joblib.load(model_path)\n",
    "    \n",
    "#     trained_models['wind_max_from_mean'] = joblib.load(os.path.join(MODEL_SAVE_DIR, 'model_wind_max.pkl'))\n",
    "#     trained_models['wind_std_from_mean'] = joblib.load(os.path.join(MODEL_SAVE_DIR, 'model_wind_std.pkl'))\n",
    "#     print(\"  All models loaded successfully.\")\n",
    "# except FileNotFoundError as e:\n",
    "#     print(f\"Error loading model file: {e.filename}. Please ensure training was successful.\")\n",
    "#     exit()\n",
    "\n",
    "\n",
    "# # --- 3. FILTER AND PREPARE NASA DATA FOR PREDICTION ---\n",
    "# print(\"\\n--- Step 2: Filtering and Preparing NASA data for Minneapolis ---\")\n",
    "\n",
    "# # Find the single closest point in the NASA data to our ERA5 point\n",
    "# nasa_df_full['lon_180'] = (nasa_df_full['lon'] + 180) % 360 - 180\n",
    "# unique_coords = nasa_df_full[['lat', 'lon', 'lon_180']].drop_duplicates()\n",
    "# dist_sq = (unique_coords['lat'] - TARGET_LAT)**2 + (unique_coords['lon_180'] - TARGET_LON)**2\n",
    "# closest_point = unique_coords.loc[dist_sq.idxmin()]\n",
    "\n",
    "# print(f\"  Filtering NASA data for closest point: Lat={closest_point['lat']}, Lon={closest_point['lon_180']:.2f}\")\n",
    "# nasa_daily_df = nasa_df_full[\n",
    "#     (nasa_df_full['lat'] == closest_point['lat']) & \n",
    "#     (nasa_df_full['lon'] == closest_point['lon'])\n",
    "# ].copy().set_index('time')\n",
    "\n",
    "# # Create the full predictor set\n",
    "# nasa_predictors_df = pd.DataFrame(index=nasa_daily_df.index)\n",
    "# for var_name, predictors in predictor_map.items():\n",
    "#     for p in predictors:\n",
    "#         # Map NASA source columns to the names the model expects\n",
    "#         source_col = p.split('_')[0] # e.g., 'tasmin' from 'air_temperature_k_min'\n",
    "#         if source_col == 'sfcWind': source_col = 'sfcWind' # Special case for wind mean\n",
    "#         if source_col == 'precip': source_col = 'pr' # Special case for precip sum\n",
    "\n",
    "#         if source_col in nasa_daily_df.columns:\n",
    "#             if source_col == 'pr':\n",
    "#                  nasa_predictors_df[p] = nasa_daily_df[source_col] * 86400\n",
    "#             else:\n",
    "#                  nasa_predictors_df[p] = nasa_daily_df[source_col]\n",
    "\n",
    "# # Use the Stage 1 models to predict max and std dev for wind\n",
    "# X_wind_mean = nasa_predictors_df[['sfcWind_mean']]\n",
    "# nasa_predictors_df['sfcWind_max'] = trained_models['wind_max_from_mean'].predict(X_wind_mean)\n",
    "# nasa_predictors_df['sfcWind_std'] = trained_models['wind_std_from_mean'].predict(X_wind_mean)\n",
    "# print(\"  NASA predictor data prepared.\")\n",
    "\n",
    "# # --- 4. GENERATE HOURLY PREDICTIONS ---\n",
    "# print(\"\\n--- Step 3: Generating hourly predictions from NASA daily data ---\")\n",
    "# final_predictions = {}\n",
    "# for var_name, predictors in predictor_map.items():\n",
    "#     if var_name not in trained_models or not trained_models[var_name] or not all(p in nasa_predictors_df.columns for p in predictors): continue\n",
    "    \n",
    "#     print(f\"  Predicting hourly values for: {var_name}...\")\n",
    "#     hourly_preds_list = []\n",
    "#     X_predict = nasa_predictors_df[predictors]\n",
    "#     for hour in range(24):\n",
    "#         model = trained_models[var_name].get(hour)\n",
    "#         if model:\n",
    "#             preds = model.predict(X_predict)\n",
    "#             hourly_preds_list.append(pd.Series(preds, index=X_predict.index, name=hour))\n",
    "#     if hourly_preds_list:\n",
    "#         var_df_wide = pd.concat(hourly_preds_list, axis=1)\n",
    "#         var_stacked = var_df_wide.stack()\n",
    "#         var_stacked.index = var_stacked.index.map(lambda x: x[0] + pd.to_timedelta(x[1], unit='h'))\n",
    "#         final_predictions[f'predicted_{var_name}'] = var_stacked\n",
    "\n",
    "# predictions_df = pd.DataFrame(final_predictions)\n",
    "# print(\"--- Hourly predictions generated successfully. ---\")\n",
    "\n",
    "\n",
    "# # --- 5. VALIDATE PREDICTIONS AND PLOT ---\n",
    "# print(\"\\n--- Step 4: Validating and visualizing results ---\")\n",
    "\n",
    "# validation_df = pd.merge(\n",
    "#     era5_df.rename(columns=lambda c: f\"actual_{c}\"), \n",
    "#     predictions_df, \n",
    "#     left_index=True, \n",
    "#     right_index=True,\n",
    "#     how=\"inner\"\n",
    "# )\n",
    "\n",
    "# print(\"  Final Validation Results (NASA-based Predictions vs. ERA5 Actuals):\")\n",
    "# for var_name in predictor_map.keys():\n",
    "#     actual_col, predicted_col = f'actual_{var_name}', f'predicted_{var_name}'\n",
    "#     if actual_col in validation_df.columns and predicted_col in validation_df.columns:\n",
    "#         temp_compare_df = validation_df[[actual_col, predicted_col]].dropna()\n",
    "#         if not temp_compare_df.empty:\n",
    "#             mae = mean_absolute_error(temp_compare_df[actual_col], temp_compare_df[predicted_col])\n",
    "#             rmse = np.sqrt(mean_squared_error(temp_compare_df[actual_col], temp_compare_df[predicted_col]))\n",
    "#             print(f\"    - {var_name}:\")\n",
    "#             print(f\"        MAE:  {mae:.4f}\")\n",
    "#             print(f\"        RMSE: {rmse:.4f}\")\n",
    "\n",
    "# # --- Plotting ---\n",
    "# start_plot_date = f'{min(SELECTED_YEARS)}-01-01'\n",
    "# end_plot_date = f'{min(SELECTED_YEARS)}-01-03'\n",
    "\n",
    "# for var_name in predictor_map.keys():\n",
    "#     # Code to generate plots as in the previous visualization script\n",
    "#     pass # Placeholder for brevity, plotting logic is the same\n",
    "\n",
    "# # --- 6. SAVE FINAL RESULTS ---\n",
    "# print(f\"\\n--- Step 5: Saving final validation results to {FINAL_VALIDATION_OUTPUT_FILE} ---\")\n",
    "# validation_df.to_csv(FINAL_VALIDATION_OUTPUT_FILE)\n",
    "# print(\"Save complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3cab9c-bd69-43ea-896b-4976ddec4916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
